#!/usr/bin/env python3
"""æ•°æ®åº“é…ç½®å’Œæ“ä½œ"""

# æ ‡å‡†åº“å¯¼å…¥
import threading
from typing import List

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
import psycopg2
import psycopg2.pool
import pandas as pd

# æœ¬åœ°æ¨¡å—å¯¼å…¥
from excel_utils import get_merged_cells, read_merged_headers
from state_standardizer import standardize_dataframe_states, standardize_state_name
from data_cleaner import *


DB_CONFIG = {
    'host': 'localhost', 'port': 5432, 'user': 'postgres', 
    'password': 'postgre', 'database': 'postgres'
}

# å…¨å±€è¿æ¥æ± 
_connection_pool = None
_pool_lock = threading.Lock()

# è¿æ¥è·Ÿè¸ªï¼ˆç”¨äºè°ƒè¯•ï¼‰
_active_connections = set()
_connections_lock = threading.Lock()

def get_connection_pool(minconn=1, maxconn=10):
    """è·å–æ•°æ®åº“è¿æ¥æ± ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _connection_pool
    
    if _connection_pool is None:
        with _pool_lock:
            if _connection_pool is None:
                try:
                    _connection_pool = psycopg2.pool.ThreadedConnectionPool(
                        minconn=minconn,
                        maxconn=maxconn,
                        **DB_CONFIG
                    )
                    print(f"âœ“PostgreSQLè¿æ¥æ± åˆ›å»ºæˆåŠŸ: {minconn}-{maxconn}ä¸ªè¿æ¥")
                    # å°è¯•å¯ç”¨PostGISæ‰©å±•ï¼ˆè‹¥å·²å¯ç”¨å°†è¢«å¿½ç•¥ï¼‰
                    try:
                        _conn = _connection_pool.getconn()
                        if _conn:
                            with _conn.cursor() as _cur:
                                _cur.execute("CREATE EXTENSION IF NOT EXISTS postgis;")
                                _conn.commit()
                                print("âœ“PostGISæ‰©å±•å·²å¯ç”¨æˆ–å·²å­˜åœ¨")
                        if _conn:
                            _connection_pool.putconn(_conn)
                    except Exception as ee:
                        print(f"âš å¯ç”¨PostGISæ‰©å±•å¤±è´¥: {ee}")
                except Exception as e:
                    print(f"âœ—PostgreSQLè¿æ¥æ± åˆ›å»ºå¤±è´¥: {e}")
                    return None
    
    return _connection_pool

def test_connection(conn):
    """æµ‹è¯•è¿æ¥æ˜¯å¦æœ‰æ•ˆ"""
    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT 1")
        return True
    except:
        return False

def track_connection(conn):
    """è·Ÿè¸ªè¿æ¥"""
    with _connections_lock:
        _active_connections.add(id(conn))

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥ï¼ˆä»è¿æ¥æ± ï¼‰"""
    pool = get_connection_pool()
    if not pool:
        return None
    
    try:
        conn = pool.getconn()
        if not conn:
            return None
            
        # æµ‹è¯•è¿æ¥æ˜¯å¦æœ‰æ•ˆ
        if test_connection(conn):
            track_connection(conn)
            return conn
        
        # è¿æ¥æ— æ•ˆï¼Œå°è¯•é‡æ–°è·å–
        print("âœ—è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œå°è¯•é‡æ–°è·å–")
        try:
            pool.putconn(conn, close=True)
        except:
            pass
            
        conn = pool.getconn()
        if conn and test_connection(conn):
            track_connection(conn)
            return conn
            
        return None
    except Exception as e:
        print(f"âœ—ä»è¿æ¥æ± è·å–è¿æ¥å¤±è´¥: {e}")
        return None

def return_db_connection(conn):
    """å½’è¿˜æ•°æ®åº“è¿æ¥åˆ°è¿æ¥æ± """
    if not conn:
        return
    
    # æ£€æŸ¥è¿æ¥æ˜¯å¦è¢«è·Ÿè¸ªï¼ˆé˜²æ­¢å½’è¿˜æœªä»æ± ä¸­è·å–çš„è¿æ¥ï¼‰
    conn_id = id(conn)
    with _connections_lock:
        if conn_id not in _active_connections:
            print("âœ—å°è¯•å½’è¿˜æœªè·Ÿè¸ªçš„è¿æ¥ï¼Œç›´æ¥å…³é—­")
            safe_close_connection(conn)
            return
        _active_connections.discard(conn_id)
    
    if not _connection_pool:
        print("âœ—è¿æ¥æ± ä¸å­˜åœ¨ï¼Œæ— æ³•å½’è¿˜è¿æ¥")
        safe_close_connection(conn)
        return
    
    try:
        # æ£€æŸ¥è¿æ¥æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
        if not test_connection(conn):
            print("âœ—è¿æ¥å·²å¤±æ•ˆï¼Œç›´æ¥å…³é—­")
            safe_close_connection(conn)
            return
        
        # å½’è¿˜è¿æ¥åˆ°æ± 
        _connection_pool.putconn(conn)
    except Exception as e:
        print(f"âœ—å½’è¿˜è¿æ¥åˆ°è¿æ¥æ± å¤±è´¥: {e}")
        safe_close_connection(conn)

def safe_close_connection(conn):
    """å®‰å…¨å…³é—­è¿æ¥"""
    try:
        conn.close()
    except:
        pass


def close_connection_pool():
    """å…³é—­è¿æ¥æ± """
    global _connection_pool
    if _connection_pool:
        _connection_pool.closeall()
        _connection_pool = None
        
        # æ¸…ç†è¿æ¥è·Ÿè¸ª
        with _connections_lock:
            _active_connections.clear()
        
        print("âœ“æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")


def table_exists(cursor, table_name: str) -> bool:
    """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
    cursor.execute("""
        SELECT EXISTS (
            SELECT FROM information_schema.tables 
            WHERE table_schema = 'public' 
            AND table_name = %s
        );
    """, (table_name,))
    return cursor.fetchone()[0]

def create_table_safe(cursor, table_name: str, create_sql: str) -> bool:
    """å®‰å…¨åˆ›å»ºè¡¨"""
    try:
        if not table_exists(cursor, table_name):
            cursor.execute(create_sql)
            print(f"âœ“è¡¨åˆ›å»ºæˆåŠŸ: {table_name}")
            return True
        else:
            print(f"âœ“è¡¨å·²å­˜åœ¨: {table_name}")
            return True
    except Exception as e:
        print(f"âœ—è¡¨åˆ›å»ºå¤±è´¥: {table_name} - {e}")
        return False

def create_nger_table_impl(cursor):
    """åˆ›å»ºNGERè¡¨çš„å®ç°ï¼ˆä½¿ç”¨è§„èŒƒåŒ–åˆ—åï¼‰"""
    
    # å®šä¹‰NGERè¡¨çš„åˆ—ç»“æ„
    column_definitions = {
        'year_label': 'TEXT',
        'start_year': 'INTEGER', 
        'stop_year': 'INTEGER',
        'facility_name': 'TEXT',
        'state': 'TEXT',
        'facility_type': 'TEXT',
        'primary_fuel': 'TEXT',
        'reporting_entity': 'TEXT',
        'controlling_corporation': 'TEXT',
        'electricity_production_gj': 'NUMERIC',
        'electricity_production_mwh': 'NUMERIC',
        'emission_intensity_tco2e_mwh': 'NUMERIC',
        'scope1_emissions_tco2e': 'NUMERIC',
        'scope2_emissions_tco2e': 'NUMERIC',
        'total_emissions_tco2e': 'NUMERIC',
        'grid_info': 'TEXT',
        'grid_connected': 'BOOLEAN',
        'important_notes': 'TEXT',
        'lat': 'NUMERIC',
        'lon': 'NUMERIC',
        'formatted_address': 'TEXT',
        'place_id': 'TEXT',
        'postcode': 'TEXT',
        'bbox_south': 'NUMERIC',
        'bbox_north': 'NUMERIC',
        'bbox_west': 'NUMERIC',
        'bbox_east': 'NUMERIC'
    }
    
    create_sql = create_table_sql_with_normalized_columns('nger_unified', column_definitions)
    ok = create_table_safe(cursor, 'nger_unified', create_sql)
    if not ok:
        return False
    
    print(f"âœ“NGERè¡¨åˆ›å»ºå®Œæˆï¼ˆè§„èŒƒåŒ–åˆ—åï¼‰")
    
    # ç¡®ä¿geometryåˆ—
    try:
        ensure_geometry_column_and_index(cursor, 'nger_unified', 'lat', 'lon', 'geom')
    except Exception as e:
        print(f"âš NGERå‡ ä½•åˆ—å¤„ç†å¤±è´¥: {e}")
    return True

def create_nger_table(conn) -> bool:
    """åˆ›å»ºNGERè¡¨ï¼ˆåœ¨å•çº¿ç¨‹ä¸­è°ƒç”¨ï¼‰"""
    try:
        cursor = conn.cursor()
        result = create_nger_table_impl(cursor)
        if result is not False:
            conn.commit()
        return result
    except Exception as e:
        print(f"âœ—NGERè¡¨åˆ›å»ºå¤±è´¥: {e}")
        conn.rollback()
        return False

def create_cer_tables_impl(cursor):
    """åˆ›å»ºCERè¡¨çš„å®ç°ï¼ˆä½¿ç”¨è§„èŒƒåŒ–åˆ—åï¼‰"""
    
    cer_table_types = ['approved_power_stations', 'committed_power_stations', 'probable_power_stations']
    
    for table_type in cer_table_types:
        # åŸºç¡€åˆ—ç»“æ„
        column_definitions = {
            'accreditation_code': 'TEXT',
            'power_station_name': 'TEXT',
            'project_name': 'TEXT', 
            'state': 'TEXT',
            'postcode': 'TEXT',
            'installed_capacity_mw': 'NUMERIC',
            'mw_capacity': 'NUMERIC',
            'fuel_source': 'TEXT',
            'accreditation_start_date': 'TEXT',
            'approval_date': 'TEXT',
            'committed_date': 'TEXT',
            'committed_date_year': 'INTEGER',
            'committed_date_month': 'INTEGER',
            'accreditation_start_date_year': 'INTEGER',
            'accreditation_start_date_month': 'INTEGER',
            'approval_date_year': 'INTEGER',
            'approval_date_month': 'INTEGER',
            # åœ°ç†ç¼–ç å­—æ®µ
            'lat': 'NUMERIC',
            'lon': 'NUMERIC',
            'formatted_address': 'TEXT',
            'place_id': 'TEXT',
            'bbox_south': 'NUMERIC',
            'bbox_north': 'NUMERIC',
            'bbox_west': 'NUMERIC',
            'bbox_east': 'NUMERIC'
        }
        
        normalized_table_name = normalize_db_column_name(f"cer_{table_type}")
        create_sql = create_table_sql_with_normalized_columns(normalized_table_name, column_definitions)
        
        if not create_table_safe(cursor, normalized_table_name, create_sql):
            return False
        
        print(f"âœ“CERè¡¨åˆ›å»ºå®Œæˆï¼ˆè§„èŒƒåŒ–åˆ—åï¼‰: {normalized_table_name}")
    
    return True

def create_cer_tables(conn) -> bool:
    """åˆ›å»ºCERè¡¨ï¼ˆåœ¨å•çº¿ç¨‹ä¸­è°ƒç”¨ï¼‰"""
    try:
        cursor = conn.cursor()
        result = create_cer_tables_impl(cursor)
        if result is not False:
            conn.commit()
        return result
    except Exception as e:
        print(f"âœ—CERè¡¨åˆ›å»ºå¤±è´¥: {e}")
        conn.rollback()
        return False


def create_all_abs_tables(conn, file_path: str) -> bool:
    """é¢„åˆ›å»ºæ‰€æœ‰ABSè¡¨ï¼ˆåœ¨å•çº¿ç¨‹ä¸­è°ƒç”¨ï¼‰"""
    try:
        cursor = conn.cursor()
        
        # åœ°ç†çº§åˆ«å®šä¹‰
        levels = {
            "Table 1": {"desc": "å·çº§", "level": 0},
            "Table 2": {"desc": "åœ°æ–¹æ”¿åºœçº§", "level": 1}
        }
        
        for sheet_name in ["Table 1", "Table 2"]:
            level_info = levels[sheet_name]
            print(f"æ­£åœ¨é¢„åˆ›å»ºABSè¡¨: {sheet_name}({level_info['desc']})...")
            
            try:
                merged_cells = get_merged_cells(file_path, sheet_name)
                df = read_merged_headers(file_path, sheet_name)
                print(f"å‘ç°{len(merged_cells)}ä¸ªåˆå¹¶å•å…ƒæ ¼éœ€è¦åˆ›å»ºè¡¨")
                
                for cell in merged_cells:
                    start_col, end_col = cell['start_col'] - 1, cell['end_col']
                    # å®‰å…¨åˆ—èŒƒå›´ï¼ˆé˜²æ­¢è¶Šç•Œï¼‰
                    total_cols = len(df.columns)
                    if total_cols < 3:
                        print(f"âœ—è·³è¿‡: åˆ—ä¸è¶³3åˆ—ï¼Œæ— æ³•æ„å»ºåŸºç¡€åˆ— Code/Label/Year -> {cell['value']}")
                        continue
                    start_col_safe = max(0, min(start_col, total_cols))
                    end_col_safe = max(start_col_safe, min(end_col, total_cols))
                    if start_col_safe >= end_col_safe:
                        print(f"âœ—è·³è¿‡: æ— æ•ˆåˆ—èŒƒå›´ [{start_col},{end_col}) -> [{start_col_safe},{end_col_safe}) : {cell['value']}")
                        continue
                    selected_cols = ['Code', 'Label', 'Year'] + list(df.columns[start_col_safe:end_col_safe])
                    
                    # åˆ›å»ºè¡¨
                    try:
                        clean_table = normalize_db_column_name(cell['value'])
                        
                        # ä½¿ç”¨è§„èŒƒåŒ–åˆ—åå’Œç±»å‹æ£€æµ‹åˆ›å»ºè¡¨
                        
                        # æ£€æµ‹åˆ—ç±»å‹ï¼ˆåŸºäºè¿™ä¸ªå­é›†çš„æ•°æ®ï¼‰
                        # åŸºäºå®‰å…¨èŒƒå›´æå–å­é›†
                        idx_slice = [0, 1, 2] + list(range(start_col_safe, end_col_safe))
                        subset_df = df.iloc[:, idx_slice]
                        subset_df.columns = selected_cols
                        column_types = detect_numeric_columns(subset_df)
                        
                        # åˆ›å»ºåˆ—åè§„èŒƒåŒ–åˆ—è¡¨ï¼ˆä¸ selected_cols ç­‰é•¿ã€é¡ºåºå¯¹é½ï¼‰
                        normalized_cols = normalize_column_mapping(selected_cols)
                        
                        # æ„å»ºåˆ—å®šä¹‰å­—å…¸
                        column_definitions = {
                            'code': 'TEXT',
                            'label': 'TEXT', 
                            'year': 'INTEGER',
                            'geographic_level': 'INTEGER',
                            'standardized_state': 'TEXT',
                            'lga_code_clean': 'TEXT',
                            'lga_name_clean': 'TEXT'
                        }
                        
                        # æ·»åŠ æ•°æ®åˆ—
                        for col, normalized_col in zip(selected_cols[3:], normalized_cols[3:]):  # è·³è¿‡Code, Label, Year
                            
                            # æ ¹æ®æ£€æµ‹åˆ°çš„ç±»å‹è®¾ç½®SQLç±»å‹
                            col_type = column_types.get(col, 'text')
                            if col_type in ['integer']:
                                sql_type = 'INTEGER'
                            elif col_type in ['float', 'percentage', 'currency']:
                                sql_type = 'NUMERIC'
                            else:
                                sql_type = 'TEXT'
                            
                            column_definitions[normalized_col] = sql_type
                        
                        # åˆ›å»ºè¡¨ï¼ˆä¸ºABSè¡¨æ·»åŠ å‰ç¼€ï¼‰
                        normalized_table_name = normalize_db_column_name(f"abs_{cell['value']}")  # ä½¿ç”¨ç»Ÿä¸€å‡½æ•°
                        create_sql = create_table_sql_with_normalized_columns(
                            normalized_table_name, 
                            column_definitions
                        )
                        
                        if not create_table_safe(cursor, normalized_table_name, create_sql):
                            print(f"âœ—ABSè¡¨åˆ›å»ºå¤±è´¥: {cell['value']}")
                            return False
                        else:
                            # æŠ¥å‘Šåˆ—åè§„èŒƒåŒ–å’Œç±»å‹æ£€æµ‹ç»“æœ
                            print_column_mapping_report(selected_cols, normalized_cols)
                            numeric_cols = {k: v for k, v in column_types.items() if v != 'text'}
                            if numeric_cols:
                                print(f"  ğŸ“Š{cell['value']}: æ£€æµ‹åˆ°{len(numeric_cols)}ä¸ªæ•°å€¼åˆ—")
                    except Exception as e:
                        print(f"âœ—ABSè¡¨åˆ›å»ºå¤±è´¥: {cell['value']} - {e}")
                        return False
                
                print(f"âœ“ABSè¡¨é¢„åˆ›å»ºå®Œæˆ: {sheet_name} - {len(merged_cells)}ä¸ªè¡¨")
                
            except Exception as e:
                print(f"âœ—ABSè¡¨é¢„åˆ›å»ºå¤±è´¥: {sheet_name} - {e}")
                return False
        
        # æˆåŠŸåˆ›å»ºåæäº¤äº‹åŠ¡ï¼Œç¡®ä¿è¡¨å®é™…å­˜åœ¨
        conn.commit()
        return True
        
    except Exception as e:
        print(f"âœ—ABSè¡¨é¢„åˆ›å»ºå¤±è´¥: {e}")
        conn.rollback()
        return False





def batch_insert(cursor, insert_sql: str, data: List[tuple], batch_size: int = 1000) -> None:
    """æ‰¹é‡æ’å…¥æ•°æ®"""
    for i in range(0, len(data), batch_size):
        cursor.executemany(insert_sql, data[i:i + batch_size])

def prepare_insert_sql(table_name: str, columns: List[str]) -> str:
    """å‡†å¤‡æ’å…¥SQLè¯­å¥"""
    placeholders = ', '.join(['%s'] * len(columns))
    return f"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({placeholders})"


# ä¸“ç”¨å‡½æ•°
def save_nger_data(conn, year_label: str, df: pd.DataFrame) -> bool:
    """ä¿å­˜NGERæ•°æ®"""
    try:
        cursor = conn.cursor()
        
        # æ ‡å‡†åŒ–å·å
        print(f"  ğŸ“æ ‡å‡†åŒ–NGERå·å...")
        standardize_dataframe_states(df, 'state')
        
        # ä½¿ç”¨è§„èŒƒåŒ–çš„åˆ—åæ˜ å°„
        mappings = {
            'facility_type': ['type'],
            'electricity_production_gj': ['electricityproductiongj'],
            'electricity_production_mwh': ['electricityproductionmwh'],
            'emission_intensity_tco2e_mwh': ['emissionintensitytco2emwh', 'emissionintensitytmwh'],
            'scope1_emissions_tco2e': ['scope1tco2e', 'totalscope1emissionstco2e'],
            'scope2_emissions_tco2e': ['scope2tco2e', 'totalscope2emissionstco2e', 'totalscope2emissionstco2e2'],
            'total_emissions_tco2e': ['totalemissionstco2e'],
            'grid_info': ['grid'],
            'grid_connected': ['gridconnected', 'gridconnected2'],
            'important_notes': ['importantnotes']
        }
        
        # è§„èŒƒåŒ–åŸå§‹åˆ—ååˆ°æ•°æ®åº“åˆ—åçš„æ˜ å°„
        column_name_mapping = {
            'facilityname': 'facility_name',
            'primaryfuel': 'primary_fuel',
            'reportingentity': 'reporting_entity',
            'controllingcorporation': 'controlling_corporation'
        }
        
        # ç¡®ä¿åœ°ç†ç¼–ç åˆ—å­˜åœ¨ï¼ˆå³ä½¿è¡¨å·²å­˜åœ¨ï¼‰
        geocode_fields = {
            'lat': 'NUMERIC',
            'lon': 'NUMERIC',
            'formatted_address': 'TEXT',
            'place_id': 'TEXT',
            'postcode': 'TEXT',
            'bbox_south': 'NUMERIC',
            'bbox_north': 'NUMERIC',
            'bbox_west': 'NUMERIC',
            'bbox_east': 'NUMERIC'
        }

        for col_name, col_type in geocode_fields.items():
            try:
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.columns 
                        WHERE table_schema = 'public' 
                        AND table_name = %s 
                        AND column_name = %s
                    );
                """, ('nger_unified', col_name))
                if not cursor.fetchone()[0]:
                    cursor.execute(f"ALTER TABLE nger_unified ADD COLUMN {col_name} {col_type}")
                    print(f"  âœ“æ·»åŠ NGERåˆ—: {col_name} ({col_type})")
            except Exception as e:
                print(f"  âš æ·»åŠ NGERåˆ—å¤±è´¥: {col_name} - {e}")

        # ç»Ÿä¸€å¸ƒå°”è§£æå‡½æ•°ï¼ˆé’ˆå¯¹ grid_connected å­—æ®µï¼‰
        def _parse_bool(value):
            try:
                if isinstance(value, bool):
                    return value
                s = str(value).strip().lower()
                truthy = {
                    'true', 'yes', '1', 'y', 't', 'connected', 'on-grid', 'on grid', 'ongrid', 'on'
                }
                falsy = {
                    'false', 'no', '0', 'n', 'f', 'not connected', 'disconnected', 'off-grid', 'off grid', 'offgrid', 'off'
                }
                if s in truthy:
                    return True
                if s in falsy:
                    return False
                return None
            except Exception:
                return None

        data = []
        for _, row in df.iterrows():
            row_data = [year_label]
            
            # æ·»åŠ æ—¶é—´åˆ—
            start_year = row.get('start_year') if 'start_year' in df.columns else None
            stop_year = row.get('stop_year') if 'stop_year' in df.columns else None
            row_data.append(start_year)
            row_data.append(stop_year)
            
            # åŸºç¡€åˆ—ï¼ˆä½¿ç”¨è§„èŒƒåŒ–çš„åˆ—åï¼‰
            basic_columns = ['facilityname', 'state', 'primaryfuel', 'reportingentity', 'controllingcorporation']
            for col in basic_columns:
                value = row.get(col) if col in df.columns else None
                has_value = (value is not None) and (not pd.isna(value)) and (str(value).strip() != '') and (str(value).strip().lower() not in {'nan', 'none', '-'})
                row_data.append(str(value).strip() if has_value else None)
            
            # æ˜ å°„åˆ—
            for target_col, source_cols in mappings.items():
                value = None
                for source_col in source_cols:
                    if source_col in df.columns:
                        val = row.get(source_col)
                        # ä¸è¦ç”¨ truthiness è¿‡æ»¤ï¼Œå¦åˆ™ä¼šæŠŠ False/0 å½“ä½œç©ºå€¼
                        has_value = (val is not None) and (not pd.isna(val)) and (str(val).strip() != '')
                        if has_value:
                            if target_col == 'grid_connected':
                                value = _parse_bool(val)
                            elif target_col.endswith(('_gj', '_mwh', '_tco2e')):
                                try:
                                    value = float(str(val).replace(',', ''))
                                except:
                                    value = None
                            else:
                                value = str(val)
                            break
                row_data.append(value)

            # è¿½åŠ åœ°ç†ç¼–ç åˆ—å€¼
            for field in geocode_fields.keys():
                value = row.get(field)
                if field in ['lat', 'lon', 'confidence'] and value is not None and not pd.isna(value):
                    try:
                        row_data.append(float(value))
                    except:
                        row_data.append(None)
                else:
                    row_data.append(str(value) if value is not None and not pd.isna(value) and str(value).strip() else None)
            data.append(tuple(row_data))
        
        # ä½¿ç”¨è§„èŒƒåŒ–çš„åˆ—å
        cols = ['year_label', 'start_year', 'stop_year', 'facility_name', 'state', 'primary_fuel', 'reporting_entity', 'controlling_corporation',
                'facility_type', 'electricity_production_gj', 'electricity_production_mwh',
                'emission_intensity_tco2e_mwh', 'scope1_emissions_tco2e', 'scope2_emissions_tco2e',
                'total_emissions_tco2e', 'grid_info', 'grid_connected', 'important_notes'] + list(geocode_fields.keys())
        
        # æ‰¹é‡æ’å…¥
        insert_sql = prepare_insert_sql('nger_unified', cols)
        batch_insert(cursor, insert_sql, data)
        
        # æ’å…¥å®Œæˆåç”Ÿæˆ/æ›´æ–°geomåˆ—
        try:
            ensure_geometry_column_and_index(cursor, 'nger_unified', 'lat', 'lon', 'geom')
            ensure_area_and_bbox_geometries(cursor, 'nger_unified',
                                                'bbox_west', 'bbox_south', 'bbox_east', 'bbox_north')
        except Exception as e:
            print(f"  âš æ›´æ–°NGERå‡ ä½•åˆ—å¤±è´¥: {e}")

        conn.commit()
        print(f"  âœ“NGERæ•°æ®å…¥åº“æˆåŠŸ: {len(data)}è¡Œ -> nger_unifiedè¡¨")
        return True
        
    except Exception as e:
        print(f"  âœ—NGERæ•°æ®å…¥åº“å¤±è´¥: {e}")
        conn.rollback()
        return False

def save_cer_data(conn, table_type: str, df: pd.DataFrame) -> bool:
    """ä¿å­˜CERæ•°æ®ï¼ˆä½¿ç”¨è§„èŒƒåŒ–åˆ—åï¼Œè¡¨å·²å­˜åœ¨ï¼‰"""
    try:
        cursor = conn.cursor()
        normalized_table_name = normalize_db_column_name(f"cer_{table_type}")
        
        # æ ‡å‡†åŒ–å·å
        print(f"  ğŸ“æ ‡å‡†åŒ–CERå·å...")
        standardize_dataframe_states(df, 'state')
        
        # åŸå§‹åˆ—å’Œåœ°ç†ç¼–ç åˆ—
        geocode_fields = {
            'lat': 'NUMERIC',
            'lon': 'NUMERIC',
            'formatted_address': 'TEXT',
            'place_id': 'TEXT',
            'postcode': 'TEXT',
            'bbox_south': 'NUMERIC',
            'bbox_north': 'NUMERIC',
            'bbox_west': 'NUMERIC',
            'bbox_east': 'NUMERIC'
        }
        geocode_column_names = set(geocode_fields.keys())
        original_cols = [col for col in df.columns if col not in geocode_column_names]
        
        # å‡†å¤‡åˆ—ä¿¡æ¯ç”¨äºæ•°æ®æ’å…¥
        used_names = {'id'}
        clean_original_cols = []
        for col in original_cols:
            clean_col = normalize_db_column_name(col)
            # é¿å…ä¸åœ°ç†ç¼–ç å­—æ®µå†²çª
            if clean_col in ['postcode', 'state_full', 'country', 'locality']:
                clean_col = f"original_{clean_col}"
            # ç¡®ä¿å”¯ä¸€æ€§
            original_name = clean_col
            counter = 1
            while clean_col in used_names:
                clean_col = f"{original_name}_{counter}"
                counter += 1
            used_names.add(clean_col)
            clean_original_cols.append(clean_col)
        
        # åŠ¨æ€æ·»åŠ åˆ—åˆ°è¡¨
        all_columns = clean_original_cols + list(geocode_fields.keys())
        for col_name in all_columns:
            try:
                # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.columns 
                        WHERE table_schema = 'public' 
                        AND table_name = %s 
                        AND column_name = %s
                    );
                """, (normalized_table_name, col_name))
                column_exists = cursor.fetchone()[0]
                
                if not column_exists:
                    # ç¡®å®šåˆ—ç±»å‹
                    if col_name in geocode_fields:
                        col_type = geocode_fields[col_name]
                    else:
                        col_type = 'TEXT'  # é»˜è®¤ä¸ºTEXTç±»å‹
                    
                    # æ·»åŠ åˆ—
                    alter_sql = f"ALTER TABLE {normalized_table_name} ADD COLUMN {col_name} {col_type}"
                    cursor.execute(alter_sql)
                    print(f"  âœ“æ·»åŠ åˆ—: {col_name} ({col_type})")
            except Exception as e:
                print(f"  âš æ·»åŠ åˆ—å¤±è´¥: {col_name} - {e}")
                # ç»§ç»­å¤„ç†å…¶ä»–åˆ—
        
        # å‡†å¤‡æ•°æ®
        data = []
        
        for _, row in df.iterrows():
            row_data = []
            for col in original_cols:
                val = row.get(col)
                has_value = (val is not None) and (not pd.isna(val)) and (str(val).strip() != '') and (str(val).strip().lower() not in {'nan', 'none', '-'})
                row_data.append(str(val).strip() if has_value else None)
            
            for field in geocode_fields.keys():
                value = row.get(field)
                if field in ['lat', 'lon', 'confidence'] and value is not None and not pd.isna(value):
                    try:
                        row_data.append(float(value))
                    except:
                        row_data.append(None)
                else:
                    has_value = (value is not None) and (not pd.isna(value)) and (str(value).strip() != '') and (str(value).strip().lower() not in {'nan', 'none', '-'})
                    row_data.append(str(value).strip() if has_value else None)
            
            data.append(tuple(row_data))
        
        # æ’å…¥
        insert_sql = prepare_insert_sql(normalized_table_name, all_columns)
        batch_insert(cursor, insert_sql, data)
        
        # ä¸ºCERè¡¨åˆ›å»º/æ›´æ–°geomåˆ—
        try:
            ensure_geometry_column_and_index(cursor, normalized_table_name, 'lat', 'lon', 'geom')
            ensure_area_and_bbox_geometries(cursor, normalized_table_name,
                                            'bbox_west', 'bbox_south', 'bbox_east', 'bbox_north')
        except Exception as e:
            print(f"  âš æ›´æ–°CERå‡ ä½•åˆ—å¤±è´¥: {e}")

        conn.commit()
        print(f"  âœ“CERæ•°æ®å…¥åº“æˆåŠŸ: {normalized_table_name} ({len(data)}è¡Œï¼Œå«åœ°ç†ç¼–ç )")
        return True
        
    except Exception as e:
        print(f"  âœ—CERæ•°æ®å…¥åº“å¤±è´¥: {e}")
        conn.rollback()
        return False

def create_abs_table(conn, merged_cell_value: str, columns: List[str]) -> str:
    """åˆ›å»ºABSè¡¨ï¼ˆè¡¨å·²å­˜åœ¨ï¼Œåªè¿”å›è¡¨åï¼‰"""
    clean_table = normalize_db_column_name(f"abs_{merged_cell_value}")
    print(f"âœ“ABSè¡¨å·²å­˜åœ¨: {clean_table}")
    return clean_table

def create_abs_table_with_types(conn, merged_cell_value: str, columns: List[str], column_types: dict) -> str:
    """åˆ›å»ºABSè¡¨ï¼ˆåŸºäºé¢„æ£€æµ‹çš„åˆ—ç±»å‹ï¼Œè¡¨å·²åœ¨é¢„åˆ›å»ºé˜¶æ®µå­˜åœ¨ï¼‰"""
    clean_table = normalize_db_column_name(f"abs_{merged_cell_value}")
    print(f"âœ“ABSè¡¨å·²å­˜åœ¨ï¼ˆå¸¦ç±»å‹ï¼‰: {clean_table}")
    return clean_table

def insert_abs_data(conn, table_name: str, df: pd.DataFrame, geo_level: int = None) -> bool:
    """æ’å…¥ABSæ•°æ®ï¼ˆåŒ…å«æ•°å€¼è½¬æ¢å’ŒLGAæ ‡å‡†åŒ–ï¼‰"""
    try:
        cursor = conn.cursor()
        
        # ä½¿ç”¨æ–°çš„ABSæ•°æ®æ¸…ç†å·¥å…·
        
        print(f"  ğŸ§¹å¼€å§‹ABSæ•°æ®æ¸…ç†...")
        
        # æ ‡å‡†åŒ–å·å
        print(f"  ğŸ“æ ‡å‡†åŒ–ABSå·å...")
        if 'Label' in df.columns:
            df['standardized_state'] = df['Label'].apply(standardize_state_name)
        
        # æ‰§è¡Œæ•°å€¼è½¬æ¢å’ŒLGAæ ‡å‡†åŒ–
        df_cleaned, column_types = process_abs_data_with_cleaning(df)
        
        # å‡†å¤‡åˆ—åæ˜ å°„
        cols = ['code', 'label', 'year', 'geographic_level']
        used = set(cols)
        
        # æ·»åŠ æ ‡å‡†åŒ–åˆ—
        if 'standardized_state' in df_cleaned.columns:
            cols.append('standardized_state')
            used.add('standardized_state')
        
        # å¤„ç†æ•°æ®åˆ—
        for col in df_cleaned.columns[3:]:
            if col == 'standardized_state':
                continue
            clean_col = normalize_db_column_name(col)
            original = clean_col
            counter = 1
            while clean_col in used:
                clean_col = f"{original}_{counter}"
                counter += 1
            used.add(clean_col)
            cols.append(clean_col)
        
        # å‡†å¤‡æ’å…¥æ•°æ®
        data = []
        for _, row in df_cleaned.iterrows():
            row_data = []
            
            # å‰3åˆ—ï¼šCode, Label, Year
            for i in range(3):
                value = row[df_cleaned.columns[i]]
                if pd.isna(value):
                    row_data.append(None)
                else:
                    str_val = str(value).strip()
                    lower_val = str_val.lower()
                    if str_val == '-' or str_val == '' or lower_val in {'nan', 'none', 'null'}:
                        row_data.append(None)
                    elif i == 0 and len(str_val) > 50:  # Codeåˆ—æˆªæ–­
                        row_data.append(str_val[:50])
                    elif i == 2:  # Yearåˆ—è½¬æ•´æ•°
                        try:
                            row_data.append(int(float(str_val)))
                        except:
                            row_data.append(None)
                    else:
                        row_data.append(str_val)
            
            # æ·»åŠ geographic_level
            row_data.append(geo_level if geo_level is not None else -1)
            
            # æ·»åŠ æ ‡å‡†åŒ–åˆ—
            if 'standardized_state' in df_cleaned.columns:
                value = row.get('standardized_state')
                row_data.append(value if value is not None and not pd.isna(value) else None)
            
            # å¤„ç†æ•°æ®åˆ—ï¼ˆå·²ç»è¿‡æ•°å€¼è½¬æ¢ï¼‰
            for col in df_cleaned.columns[3:]:
                if col == 'standardized_state':
                    continue
                    
                value = row[col]
                
                # å¯¹äºå·²è½¬æ¢çš„æ•°å€¼åˆ—ï¼Œç›´æ¥ä½¿ç”¨æ•°å€¼
                col_type = column_types.get(col, 'text')
                if col_type != 'text' and not pd.isna(value):
                    row_data.append(value)  # æ•°å€¼å·²ç»è½¬æ¢è¿‡
                elif pd.isna(value):
                    row_data.append(None)
                else:
                    # æ–‡æœ¬åˆ—çš„å¤„ç†
                    str_val = str(value).strip()
                    lower_val = str_val.lower()
                    if str_val == '-' or str_val == '' or lower_val in {'nan', 'none', 'null'}:
                        row_data.append(None)
                    else:
                        row_data.append(str_val)
            
            data.append(tuple(row_data))
        
        insert_sql = prepare_insert_sql(table_name, cols)
        batch_insert(cursor, insert_sql, data, 10000)
        
        conn.commit()
        
        # ç»Ÿè®¡æŠ¥å‘Š
        numeric_cols = {k: v for k, v in column_types.items() if v != 'text'}
        print(f"âœ“ABSæ•°æ®æ’å…¥æˆåŠŸ: {len(data)}è¡Œ")
        if numeric_cols:
            print(f"  ğŸ“ŠåŒ…å«{len(numeric_cols)}ä¸ªæ•°å€¼åˆ—")
        
        return True
        
    except Exception as e:
        print(f"âœ—ABSæ•°æ®æ’å…¥å¤±è´¥: {e}")
        conn.rollback()
        return False

def insert_abs_data_cleaned(conn, table_name: str, df: pd.DataFrame, geo_level: int = None, column_types: dict = None) -> bool:
    """æ’å…¥å·²æ¸…ç†çš„ABSæ•°æ®ï¼ˆæ•°æ®å·²åœ¨å…¥åº“å‰å®Œæˆæ¸…ç†ï¼‰"""
    try:
        cursor = conn.cursor()
        
        print(f"  ğŸ’¾æ’å…¥å·²æ¸…ç†çš„ABSæ•°æ®åˆ°: {table_name}")
        
        # å‡†å¤‡åˆ—åæ˜ å°„ï¼ˆæ•°æ®å·²ç»åŒ…å«æ ‡å‡†åŒ–åˆ—ï¼‰
        cols = ['code', 'label', 'year', 'geographic_level']
        used = set(cols)
        
        # æ·»åŠ æ ‡å‡†åŒ–åˆ—
        for std_col in ['standardized_state', 'lga_code_clean', 'lga_name_clean']:
            if std_col in df.columns:
                cols.append(std_col)
                used.add(std_col)
        
        # å¤„ç†æ•°æ®åˆ—ï¼Œå¹¶å»ºç«‹ åŸåˆ— -> è§„èŒƒåŒ–åˆ— çš„æ˜ å°„ï¼ˆä¿æŒé¡ºåºä¸å”¯ä¸€æ€§ï¼‰
        original_to_clean = {}
        for col in df.columns[3:]:
            if col in ['standardized_state', 'lga_code_clean', 'lga_name_clean']:
                continue
            clean_col = normalize_db_column_name(col)
            original = clean_col
            counter = 1
            while clean_col in used:
                clean_col = f"{original}_{counter}"
                counter += 1
            used.add(clean_col)
            cols.append(clean_col)
            original_to_clean[col] = clean_col

        # åœ¨æ’å…¥å‰ï¼Œç¡®ä¿æ‰€æœ‰åˆ—åœ¨ç›®æ ‡è¡¨ä¸­å·²ç»å­˜åœ¨ï¼ˆé˜²æ­¢åˆ—åæ˜ å°„ä¸ä¸€è‡´å¯¼è‡´çš„ç¼ºåˆ—é”™è¯¯ï¼‰
        try:
            for clean_col in cols:
                # æ£€æŸ¥åˆ—æ˜¯å¦å·²å­˜åœ¨
                cursor.execute(
                    """
                    SELECT EXISTS (
                        SELECT FROM information_schema.columns 
                        WHERE table_schema = 'public' 
                          AND table_name = %s 
                          AND column_name = %s
                    );
                    """,
                    (table_name, clean_col)
                )
                if not bool(cursor.fetchone()[0]):
                    # æ¨æ–­åˆ—ç±»å‹ï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ column_typesï¼ˆåŸºäºåŸåˆ—åï¼‰ï¼Œå¦åˆ™æ ¹æ®åˆ—åçŒœæµ‹
                    sql_type = 'TEXT'
                    if clean_col in ['code', 'label', 'standardized_state', 'lga_code_clean', 'lga_name_clean']:
                        sql_type = 'TEXT'
                    elif clean_col in ['year', 'geographic_level']:
                        sql_type = 'INTEGER'
                    else:
                        # åæŸ¥åŸåˆ—åä»¥è·å¾—ç±»å‹æç¤º
                        source_col = None
                        for orig, mapped in original_to_clean.items():
                            if mapped == clean_col:
                                source_col = orig
                                break
                        if column_types and source_col and source_col in column_types:
                            ct = column_types[source_col]
                            if ct in ['integer']:
                                sql_type = 'INTEGER'
                            elif ct in ['float', 'percentage', 'currency']:
                                sql_type = 'NUMERIC'
                            else:
                                sql_type = 'TEXT'
                        else:
                            # åŸºäºåˆ—åå¯å‘æ¨æ–­
                            lc = clean_col.lower()
                            if any(k in lc for k in ['percent', 'rate', 'ratio']):
                                sql_type = 'NUMERIC'
                            elif any(k in lc for k in ['count', 'number', 'total']):
                                sql_type = 'INTEGER'
                            else:
                                sql_type = 'TEXT'
                    try:
                        cursor.execute(f"ALTER TABLE {table_name} ADD COLUMN {clean_col} {sql_type}")
                        print(f"  âœ“æ·»åŠ ABSåˆ—: {table_name}.{clean_col} ({sql_type})")
                    except Exception as ee:
                        print(f"  âš æ·»åŠ ABSåˆ—å¤±è´¥: {table_name}.{clean_col} - {ee}")
        except Exception as ee:
            print(f"  âš ABSåˆ—æ ¡éªŒ/è¡¥å……å¤±è´¥: {ee}")
        
        # å‡†å¤‡æ’å…¥æ•°æ®ï¼ˆæ•°æ®å·²ç»æ¸…ç†è¿‡ï¼‰
        data = []
        for _, row in df.iterrows():
            row_data = []
            
            # å‰3åˆ—ï¼šCode, Label, Year
            for i in range(3):
                value = row[df.columns[i]]
                if pd.isna(value):
                    row_data.append(None)
                else:
                    str_val = str(value).strip()
                    if str_val in ['-', '', 'nan', 'none', 'null'] or str_val.lower() in ['nan', 'none', 'null']:
                        row_data.append(None)
                    elif i == 0 and len(str_val) > 50:  # Codeåˆ—æˆªæ–­
                        row_data.append(str_val[:50])
                    elif i == 2:  # Yearåˆ—è½¬æ•´æ•°
                        try:
                            row_data.append(int(float(str_val)))
                        except:
                            row_data.append(None)
                    else:
                        row_data.append(str_val)
            
            # æ·»åŠ geographic_level
            row_data.append(geo_level if geo_level is not None else -1)
            
            # æ·»åŠ æ ‡å‡†åŒ–åˆ—
            for std_col in ['standardized_state', 'lga_code_clean', 'lga_name_clean']:
                if std_col in df.columns:
                    value = row.get(std_col)
                    row_data.append(value if value is not None and not pd.isna(value) else None)
            
            # å¤„ç†æ•°æ®åˆ—ï¼ˆå·²ç»æ¸…ç†è¿‡ï¼Œç›´æ¥ä½¿ç”¨ï¼‰
            for col in df.columns[3:]:
                if col in ['standardized_state', 'lga_code_clean', 'lga_name_clean']:
                    continue
                    
                value = row[col]
                if pd.isna(value):
                    row_data.append(None)
                else:
                    # æ•°æ®å·²ç»æ¸…ç†è¿‡ï¼Œç›´æ¥ä½¿ç”¨
                    row_data.append(value)
            
            data.append(tuple(row_data))
        
        insert_sql = prepare_insert_sql(table_name, cols)
        batch_insert(cursor, insert_sql, data, 10000)
        
        conn.commit()
        
        # ç®€åŒ–çš„ç»Ÿè®¡æŠ¥å‘Š
        print(f"  âœ“ABSæ•°æ®å…¥åº“æˆåŠŸ: {len(data)}è¡Œï¼ˆå·²é¢„æ¸…ç†ï¼‰")
        
        return True
        
    except Exception as e:
        print(f"  âœ—ABSæ•°æ®å…¥åº“å¤±è´¥: {e}")
        conn.rollback()
        return False

# =============================================================================
# PostGIS/Geometry è¾…åŠ©å‡½æ•°
# =============================================================================

def geometry_column_exists(cursor, table_name: str, geom_col: str = 'geom') -> bool:
    """æ£€æŸ¥geometryåˆ—æ˜¯å¦å­˜åœ¨ã€‚"""
    cursor.execute(
        """
        SELECT EXISTS (
            SELECT 1 FROM information_schema.columns
            WHERE table_schema = 'public'
              AND table_name = %s
              AND column_name = %s
        );
        """,
        (table_name, geom_col)
    )
    return bool(cursor.fetchone()[0])


def ensure_geometry_column_and_index(cursor, table_name: str, lat_col: str = 'lat', lon_col: str = 'lon', geom_col: str = 'geom') -> None:
    """ç¡®ä¿å­˜åœ¨geometry(Point,4326)åˆ—ï¼Œå¹¶ç”±lat/lonå¡«å……ï¼Œåˆ›å»ºGiSTç´¢å¼•ã€‚"""
    # 1) æ·»åŠ geometryåˆ—
    if not geometry_column_exists(cursor, table_name, geom_col):
        try:
            cursor.execute(f"ALTER TABLE {table_name} ADD COLUMN {geom_col} geometry(Point, 4326);")
            print(f"  âœ“æ·»åŠ geometryåˆ—: {table_name}.{geom_col}")
        except Exception as e:
            # è‹¥å› æ‰©å±•æœªå¯ç”¨å¤±è´¥åˆ™ä¸Šå±‚åº”å·²å°è¯•å¯ç”¨
            raise e
    # 2) ç”¨lat/lonæ›´æ–°geomï¼ˆä»…ç©ºå€¼ï¼‰
    update_sql = f"""
        UPDATE {table_name}
        SET {geom_col} = ST_SetSRID(ST_MakePoint(NULLIF({lon_col}::text,'')::double precision,
                                                 NULLIF({lat_col}::text,'')::double precision), 4326)
        WHERE {geom_col} IS NULL AND {lat_col} IS NOT NULL AND {lon_col} IS NOT NULL;
    """
    cursor.execute(update_sql)
    # 3) åˆ›å»ºGiSTç´¢å¼•ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
    index_name = f"{table_name}_{geom_col}_gist"
    try:
        cursor.execute(f"CREATE INDEX IF NOT EXISTS {index_name} ON {table_name} USING GIST ({geom_col});")
    except Exception as e:
        # å…¼å®¹è€ç‰ˆæœ¬ Postgres æ—  IF NOT EXISTS çš„æƒ…å†µï¼šå¿½ç•¥å·²å­˜åœ¨é”™è¯¯
        try:
            cursor.execute(f"SELECT 1 FROM pg_class WHERE relname = %s;", (index_name,))
            exists = bool(cursor.fetchone())
            if not exists:
                cursor.execute(f"CREATE INDEX {index_name} ON {table_name} USING GIST ({geom_col});")
        except Exception:
            pass
    print(f"  âœ“geometryç´¢å¼•å·²ç¡®ä¿: {index_name}")

def ensure_area_and_bbox_geometries(cursor, table_name: str,
                                    bbox_w_col: str = 'bbox_west', bbox_s_col: str = 'bbox_south',
                                    bbox_e_col: str = 'bbox_east', bbox_n_col: str = 'bbox_north',
                                    bbox_geom_col: str = 'geom_bbox') -> None:
    """ç¡®ä¿bboxå¤šè¾¹å½¢å‡ ä½•åˆ—å­˜åœ¨å¹¶å¡«å……ï¼ŒåŒæ—¶åˆ›å»ºGiSTç´¢å¼•ã€‚"""
    # bboxå¤šè¾¹å½¢åˆ—
    if not geometry_column_exists(cursor, table_name, bbox_geom_col):
        cursor.execute(f"ALTER TABLE {table_name} ADD COLUMN {bbox_geom_col} geometry(Polygon, 4326);")
        print(f"  âœ“æ·»åŠ geometryåˆ—: {table_name}.{bbox_geom_col}")

    # ç”¨bboxå¡«å……bboxå¤šè¾¹å½¢ï¼ˆä»…ç©ºå€¼ï¼‰
    cursor.execute(f"""
        UPDATE {table_name}
        SET {bbox_geom_col} =
            CASE
                WHEN {bbox_w_col} IS NOT NULL AND {bbox_s_col} IS NOT NULL AND {bbox_e_col} IS NOT NULL AND {bbox_n_col} IS NOT NULL THEN
                    ST_MakeEnvelope({bbox_w_col}::double precision, {bbox_s_col}::double precision,
                                    {bbox_e_col}::double precision, {bbox_n_col}::double precision, 4326)
                ELSE {bbox_geom_col}
            END
        WHERE {bbox_geom_col} IS NULL;
    """)

    # ç´¢å¼•
    index_name = f"{table_name}_{bbox_geom_col}_gist"
    try:
        cursor.execute(f"CREATE INDEX IF NOT EXISTS {index_name} ON {table_name} USING GIST ({bbox_geom_col});")
    except Exception as e:
        try:
            cursor.execute(f"SELECT 1 FROM pg_class WHERE relname = %s;", (index_name,))
            exists = bool(cursor.fetchone())
            if not exists:
                cursor.execute(f"CREATE INDEX {index_name} ON {table_name} USING GIST ({bbox_geom_col});")
        except Exception:
            pass
    print(f"  âœ“geometryç´¢å¼•å·²ç¡®ä¿: {index_name}")