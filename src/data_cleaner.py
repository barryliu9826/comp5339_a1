#!/usr/bin/env python3
"""
ç»Ÿä¸€æ•°æ®æ¸…ç†æ¨¡å—
æ•´åˆæ‰€æœ‰æ•°æ®æ¸…ç†ã€è§„èŒƒåŒ–å’Œè´¨é‡ä¿®å¤åŠŸèƒ½
"""

import pandas as pd
import re
from typing import Dict, List, Set, Optional, Any, Tuple


# =============================================================================
# å¸¸é‡å®šä¹‰
# =============================================================================

# æœˆä»½ç¼©å†™åˆ°æ•°å­—çš„æ˜ å°„ï¼ˆé€šç”¨ï¼‰
MONTH_ABBR_TO_NUM = {
    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,
    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12
}

# ç¼ºå¤±å€¼æ ‡è¯†ï¼ˆé€šç”¨ï¼‰
MISSING_VALUE_INDICATORS = ['-', '', 'nan', 'NaN', 'none', 'None', 'NULL', 'null', 'N/A', 'n/a']


# =============================================================================
# é€šç”¨è¾…åŠ©å‡½æ•°
# =============================================================================

def is_missing_value(value: Any) -> bool:
    """
    æ£€æŸ¥å€¼æ˜¯å¦ä¸ºç¼ºå¤±å€¼
    
    Args:
        value: è¦æ£€æŸ¥çš„å€¼
        
    Returns:
        Trueå¦‚æœæ˜¯ç¼ºå¤±å€¼ï¼ŒFalseå¦åˆ™
    """
    if pd.isna(value) or value is None:
        return True
    
    str_val = str(value).strip()
    return str_val in MISSING_VALUE_INDICATORS or str_val.lower() in [x.lower() for x in MISSING_VALUE_INDICATORS]


# =============================================================================
# æ•°æ®åº“åˆ—åè§„èŒƒåŒ–åŠŸèƒ½ (åŸ db_column_normalizer.py)
# =============================================================================

def normalize_db_column_name(name: str, reserved_words: Set[str] = None) -> str:
    """
    è§„èŒƒåŒ–æ•°æ®åº“åˆ—å
    
    Args:
        name: åŸå§‹åˆ—å
        reserved_words: æ•°æ®åº“ä¿ç•™å­—é›†åˆ
        
    Returns:
        è§„èŒƒåŒ–åçš„åˆ—å
    """
    if not name or str(name).strip() == '':
        return 'unnamed_column'
    
    # é»˜è®¤çš„PostgreSQLä¿ç•™å­—
    if reserved_words is None:
        reserved_words = {
            'user', 'order', 'group', 'select', 'from', 'where', 'insert', 'update', 
            'delete', 'create', 'drop', 'alter', 'table', 'index', 'view', 'database',
            'schema', 'primary', 'foreign', 'key', 'constraint', 'references', 'check',
            'unique', 'not', 'null', 'default', 'auto_increment', 'serial', 'boolean',
            'integer', 'varchar', 'text', 'date', 'time', 'timestamp', 'numeric',
            'real', 'double', 'precision', 'decimal', 'char', 'binary', 'blob'
        }
    
    # ç¬¬1æ­¥ï¼šåŸºç¡€æ¸…ç†
    clean_name = str(name).strip()
    
    # ç¬¬2æ­¥ï¼šè½¬æ¢ä¸ºå°å†™
    clean_name = clean_name.lower()
    
    # ç¬¬3æ­¥ï¼šå¤„ç†ç‰¹æ®Šå­—ç¬¦å’Œç¼©å†™
    # å¸¸è§çš„å•ä½å’Œç¼©å†™è§„èŒƒåŒ–
    unit_replacements = {
        r'\(mw\)': '_mw',
        r'\(gj\)': '_gj', 
        r'\(mwh\)': '_mwh',
        r'\(tco2e\)': '_tco2e',
        r'\(s\)': 's',
        r'\(%\)': '_percent',
        r'\$': 'dollar_',
    }
    
    for pattern, replacement in unit_replacements.items():
        clean_name = re.sub(pattern, replacement, clean_name)
    
    # ç¬¬4æ­¥ï¼šç§»é™¤å…¶ä»–ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™å­—æ¯æ•°å­—å’Œç©ºæ ¼
    clean_name = re.sub(r'[^\w\s]', '', clean_name)
    
    # ç¬¬5æ­¥ï¼šç©ºæ ¼è½¬ä¸‹åˆ’çº¿
    clean_name = re.sub(r'\s+', '_', clean_name)
    
    # ç¬¬6æ­¥ï¼šå¤šä¸ªä¸‹åˆ’çº¿åˆå¹¶ä¸ºä¸€ä¸ª
    clean_name = re.sub(r'_+', '_', clean_name)
    
    # ç¬¬7æ­¥ï¼šå»é™¤é¦–å°¾ä¸‹åˆ’çº¿
    clean_name = clean_name.strip('_')
    
    # ç¬¬8æ­¥ï¼šå¤„ç†ç©ºç»“æœ
    if not clean_name:
        clean_name = 'unnamed_column'
    
    # ç¬¬9æ­¥ï¼šå¦‚æœä»¥æ•°å­—å¼€å¤´ï¼Œæ·»åŠ å‰ç¼€
    if clean_name[0].isdigit():
        clean_name = f'col_{clean_name}'
    
    # ç¬¬10æ­¥ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºä¿ç•™å­—
    if clean_name.lower() in reserved_words:
        clean_name = f'{clean_name}_col'
    
    # ç¬¬11æ­¥ï¼šé•¿åº¦é™åˆ¶ï¼ˆPostgreSQLæ ‡è¯†ç¬¦é™åˆ¶ä¸º63å­—ç¬¦ï¼‰
    if len(clean_name) > 60:  # ç•™3ä¸ªå­—ç¬¦ç»™å¯èƒ½çš„åç¼€
        clean_name = clean_name[:60]
    
    return clean_name


def normalize_column_mapping(columns: List[str]) -> List[str]:
    """
    æŒ‰è¾“å…¥é¡ºåºè¿”å›ç­‰é•¿çš„è§„èŒƒåŒ–åˆ—ååˆ—è¡¨ï¼ˆç¡®ä¿å”¯ä¸€æ€§ï¼‰
    
    Args:
        columns: åŸå§‹åˆ—ååˆ—è¡¨
        
    Returns:
        ä¸è¾“å…¥ç­‰é•¿çš„è§„èŒƒåŒ–åˆ—ååˆ—è¡¨ï¼ˆä½ç½®å¯¹é½ï¼‰ï¼Œä¼šå¯¹é‡å¤åæ·»åŠ  _1/_2 åç¼€
    """
    normalized_list = []
    used_names = set()
    
    for original_col in columns:
        normalized = normalize_db_column_name(original_col)
        
        # å¤„ç†é‡å¤çš„è§„èŒƒåŒ–åç§°ï¼ˆåŸºäºå‡ºç°é¡ºåºï¼‰
        if normalized in used_names:
            counter = 1
            base_name = normalized
            while f"{base_name}_{counter}" in used_names:
                counter += 1
            normalized = f"{base_name}_{counter}"
        
        used_names.add(normalized)
        normalized_list.append(normalized)
    
    return normalized_list


def create_table_sql_with_normalized_columns(table_name: str, 
                                           column_definitions: Dict[str, str],
                                           primary_key: str = 'id',
                                           additional_constraints: List[str] = None) -> str:
    """
    åˆ›å»ºå¸¦æœ‰è§„èŒƒåŒ–åˆ—åçš„å»ºè¡¨SQL
    
    Args:
        table_name: è¡¨å
        column_definitions: {è§„èŒƒåŒ–åˆ—å: SQLç±»å‹} å­—å…¸
        primary_key: ä¸»é”®åˆ—å
        additional_constraints: é¢å¤–çš„çº¦æŸæ¡ä»¶
        
    Returns:
        å»ºè¡¨SQLè¯­å¥
    """
    # è§„èŒƒåŒ–è¡¨å
    normalized_table_name = normalize_db_column_name(table_name)
    
    # æ„å»ºåˆ—å®šä¹‰ï¼ˆç¡®ä¿åˆ—åå”¯ä¸€ï¼‰
    column_parts = []
    used_norm_cols = set()
    
    # ä¸»é”®
    pk_norm = normalize_db_column_name(primary_key)
    if primary_key not in column_definitions:
        column_parts.append(f"{pk_norm} SERIAL PRIMARY KEY")
        used_norm_cols.add(pk_norm)
    
    # å…¶ä»–åˆ—
    for col_name, col_type in column_definitions.items():
        norm = normalize_db_column_name(col_name)
        base = norm
        if norm in used_norm_cols:
            counter = 1
            while f"{base}_{counter}" in used_norm_cols:
                counter += 1
            norm = f"{base}_{counter}"
        used_norm_cols.add(norm)
        column_parts.append(f"{norm} {col_type}")
    
    # é¢å¤–çº¦æŸ
    if additional_constraints:
        column_parts.extend(additional_constraints)
    
    sql = f"CREATE TABLE IF NOT EXISTS {normalized_table_name} (\n"
    sql += ",\n".join(f"    {part}" for part in column_parts)
    sql += "\n);"
    
    return sql


def get_standard_column_types() -> Dict[str, str]:
    """
    è·å–æ ‡å‡†çš„åˆ—ç±»å‹æ˜ å°„
    
    Returns:
        {åˆ—åæ¨¡å¼: SQLç±»å‹} å­—å…¸
    """
    return {
        # åŸºç¡€å­—æ®µ
        'id': 'SERIAL PRIMARY KEY',
        'name': 'TEXT',
        'code': 'TEXT',
        'label': 'TEXT',
        'state': 'TEXT',
        'postcode': 'TEXT',
        
        # æ—¶é—´å­—æ®µ
        'year': 'INTEGER',
        'month': 'INTEGER',
        'start_year': 'INTEGER',
        'stop_year': 'INTEGER',
        'date': 'DATE',
        'timestamp': 'TIMESTAMP',
        
        # æ•°å€¼å­—æ®µ
        'capacity_mw': 'NUMERIC',
        'production_gj': 'NUMERIC',
        'production_mwh': 'NUMERIC',
        'emissions_tco2e': 'NUMERIC',
        'intensity_tco2e_mwh': 'NUMERIC',
        'percent': 'NUMERIC',
        'dollar': 'NUMERIC',
        'count': 'INTEGER',
        'population': 'INTEGER',
        
        # å¸ƒå°”å­—æ®µ
        'connected': 'BOOLEAN',
        'active': 'BOOLEAN',
        'enabled': 'BOOLEAN',
        
        # åœ°ç†å­—æ®µ
        'lat': 'NUMERIC',
        'lon': 'NUMERIC',
        'latitude': 'NUMERIC',
        'longitude': 'NUMERIC',
        'bbox_north': 'NUMERIC',
        'bbox_south': 'NUMERIC',
        'bbox_east': 'NUMERIC',
        'bbox_west': 'NUMERIC',
        'formatted_address': 'TEXT',
        'place_id': 'TEXT',
        
        # é»˜è®¤æ–‡æœ¬ç±»å‹
        'default': 'TEXT'
    }


def infer_column_type(column_name: str, sample_values: List = None) -> str:
    """
    æ¨æ–­åˆ—çš„SQLç±»å‹
    
    Args:
        column_name: è§„èŒƒåŒ–åçš„åˆ—å
        sample_values: æ ·æœ¬å€¼åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        æ¨æ–­çš„SQLç±»å‹
    """
    standard_types = get_standard_column_types()
    
    # ç²¾ç¡®åŒ¹é…
    if column_name in standard_types:
        return standard_types[column_name]
    
    # æ¨¡å¼åŒ¹é…
    col_lower = column_name.lower()
    
    # æ•°å€¼ç±»å‹
    if any(pattern in col_lower for pattern in ['capacity', 'mw', 'gj', 'mwh', 'tco2e', 'emissions', 'production']):
        return 'NUMERIC'
    
    if any(pattern in col_lower for pattern in ['percent', 'rate', 'ratio', 'intensity']):
        return 'NUMERIC'
    
    if any(pattern in col_lower for pattern in ['dollar', 'gdp', 'income', 'revenue', 'cost']):
        return 'NUMERIC'
    
    if any(pattern in col_lower for pattern in ['count', 'number', 'population', 'total']):
        return 'INTEGER'
    
    # æ—¶é—´ç±»å‹
    if any(pattern in col_lower for pattern in ['year', 'month', 'day']):
        return 'INTEGER'
    
    if any(pattern in col_lower for pattern in ['date', 'time']):
        return 'DATE'
    
    # å¸ƒå°”ç±»å‹
    if any(pattern in col_lower for pattern in ['connected', 'active', 'enabled', 'grid']):
        return 'BOOLEAN'
    
    # åœ°ç†ç±»å‹
    if any(pattern in col_lower for pattern in ['lat', 'lon', 'latitude', 'longitude', 'bbox']):
        return 'NUMERIC'
    
    # é»˜è®¤æ–‡æœ¬ç±»å‹
    return 'TEXT'


def print_column_mapping_report(original_columns: List[str], normalized_columns: List[str]):
    """
    æ‰“å°åˆ—åè§„èŒƒåŒ–æŠ¥å‘Šï¼ˆåŸºäºç­‰é•¿çš„åŸå§‹/è§„èŒƒåŒ–åˆ—ååˆ—è¡¨ï¼‰
    
    Args:
        original_columns: åŸå§‹åˆ—ååˆ—è¡¨
        normalized_columns: ä½ç½®å¯¹é½çš„è§„èŒƒåŒ–åˆ—ååˆ—è¡¨
    """
    print(f"ğŸ“‹ åˆ—åè§„èŒƒåŒ–æŠ¥å‘Š: {len(original_columns)} ä¸ªåˆ—")
    
    changes = []
    unchanged = []
    
    for orig_col, norm_col in zip(original_columns, normalized_columns):
        if orig_col != norm_col:
            changes.append((orig_col, norm_col))
        else:
            unchanged.append(orig_col)
    
    if changes:
        print(f"  âœ“ {len(changes)} ä¸ªåˆ—åå·²è§„èŒƒåŒ–:")
        for orig, norm in changes[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"    - {orig} â†’ {norm}")
        if len(changes) > 10:
            print(f"    ... è¿˜æœ‰ {len(changes) - 10} ä¸ªåˆ—åå˜åŒ–")
    
    if unchanged:
        print(f"  âœ“ {len(unchanged)} ä¸ªåˆ—åæ— éœ€æ›´æ”¹")
    
    print()


# =============================================================================
# ABSæ•°æ®æ¸…ç†åŠŸèƒ½ (åŸ abs_data_cleaner.py)
# =============================================================================

def detect_numeric_columns(df: pd.DataFrame, start_col: int = 3) -> Dict[str, str]:
    """
    æ£€æµ‹æ•°å€¼åˆ—ç±»å‹å¹¶è¿”å›åˆ—ååˆ°ç±»å‹çš„æ˜ å°„
    
    Args:
        df: DataFrame
        start_col: ä»ç¬¬å‡ åˆ—å¼€å§‹æ£€æµ‹ï¼ˆå‰é¢é€šå¸¸æ˜¯Code/Label/Yearï¼‰
        
    Returns:
        dict: {åˆ—å: æ•°æ®ç±»å‹} å…¶ä¸­ç±»å‹ä¸º 'integer', 'float', 'percentage', 'currency', 'text'
    """
    column_types = {}
    
    for col in df.columns[start_col:]:
        if col in ['standardized_state', 'lga_code_clean', 'lga_name_clean']:
            continue
            
        # é‡‡æ ·å‰100ä¸ªéç©ºå€¼è¿›è¡Œç±»å‹åˆ¤æ–­
        sample_values = df[col].dropna().head(100)
        if sample_values.empty:
            column_types[col] = 'text'
            continue
            
        # è½¬ä¸ºå­—ç¬¦ä¸²å¹¶æ¸…ç†
        str_values = [str(v).strip() for v in sample_values if not is_missing_value(v)]
        if not str_values:
            column_types[col] = 'text'
            continue
            
        # åˆ†ææ¨¡å¼
        numeric_count = 0
        percentage_count = 0
        currency_count = 0
        
        for val in str_values[:50]:  # åªçœ‹å‰50ä¸ªæœ‰æ•ˆå€¼
            # ç™¾åˆ†æ¯”æ£€æµ‹
            if '%' in val or 'percent' in val.lower():
                percentage_count += 1
                continue
                
            # è´§å¸æ£€æµ‹
            if any(symbol in val for symbol in ['$', 'â‚¬', 'Â£', 'Â¥', 'AUD', 'USD']):
                currency_count += 1
                continue
                
            # æ•°å€¼æ£€æµ‹ï¼ˆåŒ…å«åƒåˆ†ä½é€—å·ï¼‰
            clean_val = re.sub(r'[,\s]', '', val)  # ç§»é™¤é€—å·å’Œç©ºæ ¼
            try:
                if '.' in clean_val:
                    float(clean_val)
                    numeric_count += 1
                else:
                    int(clean_val)
                    numeric_count += 1
            except ValueError:
                pass
        
        total_checked = len(str_values[:50])
        numeric_ratio = numeric_count / total_checked if total_checked > 0 else 0
        
        # ç±»å‹åˆ¤æ–­
        if percentage_count > total_checked * 0.3:
            column_types[col] = 'percentage'
        elif currency_count > total_checked * 0.3:
            column_types[col] = 'currency'
        elif numeric_ratio > 0.7:
            # è¿›ä¸€æ­¥åˆ¤æ–­æ•´æ•°è¿˜æ˜¯æµ®ç‚¹æ•°
            has_decimal = any('.' in re.sub(r'[,\s]', '', str(v)) for v in str_values[:20] if str(v).strip())
            column_types[col] = 'float' if has_decimal else 'integer'
        else:
            column_types[col] = 'text'
    
    return column_types


def clean_numeric_value(value: Any, target_type: str = 'float') -> Optional[float]:
    """
    é€šç”¨æ•°å€¼æ¸…ç†å’Œè½¬æ¢å‡½æ•°
    
    Args:
        value: åŸå§‹å€¼
        target_type: ç›®æ ‡ç±»å‹ ('integer', 'float', 'percentage', 'currency', 'capacity')
        
    Returns:
        è½¬æ¢åçš„æ•°å€¼æˆ–None
    """
    if pd.isna(value):
        return None
        
    if is_missing_value(value):
        return None
    
    str_val = str(value).strip()
    
    try:
        # å¤„ç†ç™¾åˆ†æ¯”
        if target_type == 'percentage':
            # ç§»é™¤%ç¬¦å·ï¼Œè½¬æ¢ä¸ºå°æ•°
            clean_val = re.sub(r'[%\s]', '', str_val)
            clean_val = re.sub(r'[,]', '', clean_val)  # ç§»é™¤åƒåˆ†ä½
            return float(clean_val) / 100.0
        
        # å¤„ç†è´§å¸
        elif target_type == 'currency':
            # ç§»é™¤è´§å¸ç¬¦å·å’Œåƒåˆ†ä½
            clean_val = re.sub(r'[$â‚¬Â£Â¥,\s]|AUD|USD|EUR|GBP', '', str_val, flags=re.IGNORECASE)
            return float(clean_val)
        
        # å¤„ç†å®¹é‡ï¼ˆç§»é™¤å•ä½æ ‡è¯†ï¼‰
        elif target_type == 'capacity':
            # ç§»é™¤åƒåˆ†ä½é€—å·ã€ç©ºæ ¼å’Œå•ä½æ ‡è¯†ï¼ˆå¦‚MW, mwç­‰ï¼‰
            clean_val = re.sub(r'[,\s]', '', str_val)
            clean_val = re.sub(r'[a-zA-Z]+', '', clean_val)
            return float(clean_val)
        
        # å¤„ç†æ™®é€šæ•°å€¼
        else:
            # ç§»é™¤åƒåˆ†ä½é€—å·å’Œå¤šä½™ç©ºæ ¼
            clean_val = re.sub(r'[,\s]', '', str_val)
            if target_type == 'integer':
                return float(int(float(clean_val)))  # å…ˆè½¬floatå†è½¬inté¿å…å°æ•°ç‚¹é—®é¢˜
            else:  # float
                return float(clean_val)
                
    except (ValueError, TypeError):
        return None




def process_abs_data_with_cleaning(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, str]]:
    """
    å¤„ç†ABSæ•°æ®ï¼ŒåŒ…æ‹¬æ•°å€¼è½¬æ¢
    
    Args:
        df: åŸå§‹DataFrame
        
    Returns:
        (å¤„ç†åçš„DataFrame, åˆ—ç±»å‹æ˜ å°„)
    """
    print("  ğŸ”æ£€æµ‹ABSæ•°å€¼åˆ—ç±»å‹...")
    
    # 1. æ£€æµ‹æ•°å€¼åˆ—ç±»å‹
    column_types = detect_numeric_columns(df)
    numeric_cols = {k: v for k, v in column_types.items() if v != 'text'}
    
    if numeric_cols:
        print(f"  âœ“æ£€æµ‹åˆ°{len(numeric_cols)}ä¸ªæ•°å€¼åˆ—: {list(numeric_cols.keys())}")
        for col, col_type in list(numeric_cols.items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"    - {col}: {col_type}")
        if len(numeric_cols) > 5:
            print(f"    ... è¿˜æœ‰{len(numeric_cols)-5}ä¸ªæ•°å€¼åˆ—")
    
    # 2. è½¬æ¢æ•°å€¼åˆ—
    print("  ğŸ”¢è½¬æ¢æ•°å€¼åˆ—...")
    df_processed = df.copy()
    converted_count = 0
    for col, col_type in column_types.items():
        if col_type != 'text' and col in df_processed.columns:
            # åˆ›å»ºæ–°çš„æ•°å€¼åˆ—
            numeric_col = f"{col}_numeric"
            df_processed[numeric_col] = df_processed[col].apply(
                lambda x: clean_numeric_value(x, col_type)
            )
            
            # ç»Ÿè®¡æˆåŠŸè½¬æ¢çš„æ•°é‡
            success_count = df_processed[numeric_col].notna().sum()
            if success_count > 0:
                converted_count += success_count
                # å°†åŸåˆ—æ›¿æ¢ä¸ºæ•°å€¼åˆ—
                df_processed[col] = df_processed[numeric_col]
                df_processed.drop(columns=[numeric_col], inplace=True)
    
    if converted_count > 0:
        print(f"  âœ“æ•°å€¼è½¬æ¢å®Œæˆ: {converted_count}ä¸ªå€¼æˆåŠŸè½¬æ¢")
    else:
        print("  âš ï¸æœªå‘ç°éœ€è¦è½¬æ¢çš„æ•°å€¼")
    
    return df_processed, column_types


# =============================================================================
# é€šç”¨è¾…åŠ©å‡½æ•°
# =============================================================================

def standardize_fuel_type(value: Any) -> Optional[str]:
    """
    é€šç”¨ç‡ƒæ–™ç±»å‹æ ‡å‡†åŒ–å‡½æ•°
    
    Args:
        value: åŸå§‹ç‡ƒæ–™ç±»å‹å€¼
        
    Returns:
        æ ‡å‡†åŒ–åçš„ç‡ƒæ–™ç±»å‹åç§°
    """
    if is_missing_value(value):
        return None
    
    val_str = str(value).strip().lower()
    
    # ç»Ÿä¸€çš„ç‡ƒæ–™ç±»å‹æ˜ å°„
    fuel_mapping = {
        # å¯å†ç”Ÿèƒ½æº
        'solar': 'Solar',
        'wind': 'Wind',
        'hydro': 'Hydro',
        'biomass': 'Biomass',
        'biofuel': 'Biofuel',
        'bagasse': 'Bagasse',
        'wood': 'Biomass',
        
        # åŒ–çŸ³ç‡ƒæ–™
        'coal': 'Coal',
        'black coal': 'Black Coal',
        'brown coal': 'Brown Coal',
        'gas': 'Natural Gas',
        'natural gas': 'Natural Gas',
        'diesel': 'Diesel',
        
        # ç‰¹æ®Šç‡ƒæ–™
        'coal seam methane': 'Coal Seam Gas',
        'coal seam gas': 'Coal Seam Gas',
        'waste coal mine gas': 'Coal Mine Gas',
        'coal mine gas': 'Coal Mine Gas',
        'landfill gas': 'Landfill Gas',
        
        # å‚¨èƒ½
        'battery': 'Battery Storage',
        'battery storage': 'Battery Storage',
    }
    
    # ç›´æ¥åŒ¹é…
    if val_str in fuel_mapping:
        return fuel_mapping[val_str]
    
    # éƒ¨åˆ†åŒ¹é…
    for key, value in fuel_mapping.items():
        if key in val_str:
            return value
    
    # é»˜è®¤è¿”å›é¦–å­—æ¯å¤§å†™çš„æ ¼å¼
    return str(value).strip().title()


def clean_facility_name(value: Any, name_type: str = 'facility') -> Optional[str]:
    """
    é€šç”¨è®¾æ–½/ç”µç«™åç§°æ¸…ç†å‡½æ•°
    
    Args:
        value: åŸå§‹åç§°å€¼
        name_type: åç§°ç±»å‹ ('facility', 'station', 'project')
        
    Returns:
        æ¸…ç†åçš„åç§°
    """
    if is_missing_value(value):
        return None
    
    name = str(value).strip()
    
    # è·³è¿‡ç‰¹æ®Šçš„æ±‡æ€»è¡Œ
    if name.lower() in ['corporate total', 'facility', 'total', 'summary']:
        return name
    
    if name_type == 'station':
        # CERç”µç«™åç§°ç‰¹æ®Šå¤„ç†ï¼šç§»é™¤å†—ä½™çš„æè¿°æ€§åç¼€
        patterns_to_remove = [
            r'\s*-\s*(Solar|Wind|Gas|Hydro|Battery)\s*(w\s*SGU)?\s*-\s*[A-Z]{2,3}$',
            r'\s*-\s*(Solar|Wind|Gas|Hydro|Battery)$',
            r'\s*w\s*SGU\s*$',
            r'\s*wSGU\s*$'
        ]
        
        for pattern in patterns_to_remove:
            name = re.sub(pattern, '', name, flags=re.IGNORECASE)
    
    # é€šç”¨æ¸…ç†
    # æ ‡å‡†åŒ–åˆ†éš”ç¬¦
    name = re.sub(r'\s*-\s*', ' - ', name)
    name = re.sub(r'\s*,\s*', ', ', name)
    
    # æ ‡å‡†åŒ–æ‹¬å·æ ¼å¼
    name = re.sub(r'\s*\(\s*([^)]+)\s*\)\s*', r' (\1)', name)
    
    # æ¸…ç†å¤šä½™ç©ºæ ¼
    name = re.sub(r'\s+', ' ', name).strip()
    
    return name


# =============================================================================
# CERæ•°æ®æ¸…ç†åŠŸèƒ½ (åŸ cer_data_cleaner.py)
# =============================================================================

def normalize_cer_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """
    è§„èŒƒåŒ–CERæ•°æ®çš„åˆ—å
    
    Args:
        df: åŸå§‹DataFrame
        
    Returns:
        åˆ—åè§„èŒƒåŒ–åçš„DataFrame
    """
    df_normalized = df.copy()
    
    # åˆ—åæ˜ å°„è§„åˆ™
    column_mappings = {
        # åŸºç¡€ä¿¡æ¯åˆ—
        'accreditation code': 'accreditation_code',
        'power station name': 'power_station_name',
        'project name': 'project_name',
        'state ': 'state',  # å¤„ç†å°¾éƒ¨ç©ºæ ¼
        'state': 'state',
        'postcode': 'postcode',
        
        # å®¹é‡ç›¸å…³
        'installed capacity (mw)': 'installed_capacity_mw',
        'mw capacity': 'mw_capacity',
        
        # ç‡ƒæ–™ç±»å‹
        'fuel source (s)': 'fuel_source',
        'fuel source': 'fuel_source',
        
        # æ—¥æœŸç›¸å…³
        'accreditation start date': 'accreditation_start_date',
        'approval date': 'approval_date',
        'committed date (month/year)': 'committed_date',
    }
    
    # åˆ›å»ºæ–°çš„åˆ—åæ˜ å°„
    new_columns = {}
    for col in df.columns:
        # æ¸…ç†åˆ—åï¼šå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œè½¬å°å†™
        clean_col = str(col).strip().lower()
        
        # æŸ¥æ‰¾æ˜ å°„
        if clean_col in column_mappings:
            new_columns[col] = column_mappings[clean_col]
        else:
            # é»˜è®¤è§„èŒƒåŒ–ï¼šç©ºæ ¼è½¬ä¸‹åˆ’çº¿ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦
            normalized = re.sub(r'[^\w\s]', '', clean_col)  # å»é™¤ç‰¹æ®Šå­—ç¬¦
            normalized = re.sub(r'\s+', '_', normalized)    # ç©ºæ ¼è½¬ä¸‹åˆ’çº¿
            normalized = re.sub(r'_+', '_', normalized)     # å¤šä¸ªä¸‹åˆ’çº¿åˆå¹¶
            normalized = normalized.strip('_')              # å»é™¤é¦–å°¾ä¸‹åˆ’çº¿
            new_columns[col] = normalized
    
    # é‡å‘½ååˆ—
    df_normalized = df_normalized.rename(columns=new_columns)
    
    print(f"  âœ“CERåˆ—åè§„èŒƒåŒ–å®Œæˆ: {len(new_columns)}ä¸ªåˆ—")
    
    # æ˜¾ç¤ºä¸»è¦çš„åˆ—åå˜åŒ–
    important_changes = []
    for old_col, new_col in new_columns.items():
        if old_col != new_col and any(key in old_col.lower() for key in ['state', 'power', 'capacity', 'fuel']):
            important_changes.append(f"{old_col} â†’ {new_col}")
    
    if important_changes:
        print("  ä¸»è¦åˆ—åå˜åŒ–:")
        for change in important_changes[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"    - {change}")
    
    return df_normalized


def process_cer_time_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    å¤„ç†CERæ•°æ®çš„æ—¶é—´åˆ—ï¼Œå°†MMM-YYYYæ ¼å¼æ‹†åˆ†ä¸ºyearå’Œmonthåˆ—
    
    Args:
        df: DataFrame
        
    Returns:
        æ·»åŠ äº†yearå’Œmonthåˆ—çš„DataFrame
    """
    df_processed = df.copy()
    
    # ä½¿ç”¨é€šç”¨æœˆä»½æ˜ å°„
    month_abbr_to_num = MONTH_ABBR_TO_NUM
    
    # æŸ¥æ‰¾åŒ…å«æ—¥æœŸçš„åˆ—
    date_columns = []
    for col in df_processed.columns:
        if any(keyword in col.lower() for keyword in ['date', 'committed']):
            date_columns.append(col)
    
    processed_count = 0
    
    for date_col in date_columns:
        # åˆ›å»ºyearå’Œmonthåˆ—
        year_col = f"{date_col}_year"
        month_col = f"{date_col}_month"
        
        def parse_mmm_yyyy(date_str):
            """è§£æMMM-YYYYæ ¼å¼çš„æ—¥æœŸ"""
            if pd.isna(date_str) or not date_str:
                return None, None
            
            try:
                date_str = str(date_str).strip().lower()
                if '-' in date_str:
                    parts = date_str.split('-')
                    if len(parts) == 2:
                        month_abbr = parts[0].strip()
                        year_str = parts[1].strip()
                        
                        # è½¬æ¢æœˆä»½
                        month_num = month_abbr_to_num.get(month_abbr)
                        if month_num is None:
                            return None, None
                        
                        # è½¬æ¢å¹´ä»½
                        try:
                            year_num = int(year_str)
                            return year_num, month_num
                        except ValueError:
                            return None, None
                return None, None
            except Exception:
                return None, None
        
        # åº”ç”¨è§£æå‡½æ•°
        parsed_dates = df_processed[date_col].apply(parse_mmm_yyyy)
        
        # æ‹†åˆ†å¹´ä»½å’Œæœˆä»½
        years = [item[0] if item[0] is not None else None for item in parsed_dates]
        months = [item[1] if item[1] is not None else None for item in parsed_dates]
        
        df_processed[year_col] = years
        df_processed[month_col] = months
        
        # ç»Ÿè®¡æˆåŠŸè½¬æ¢çš„æ•°é‡
        success_count = sum(1 for y, m in zip(years, months) if y is not None and m is not None)
        if success_count > 0:
            processed_count += success_count
            print(f"  âœ“æ—¶é—´åˆ—å¤„ç†: {date_col} â†’ {year_col}, {month_col} ({success_count}æ¡è®°å½•)")
    
    if processed_count == 0:
        print("  âš ï¸æœªæ‰¾åˆ°éœ€è¦å¤„ç†çš„æ—¶é—´åˆ—")
    else:
        print(f"  âœ“CERæ—¶é—´å¤„ç†å®Œæˆ: å…±å¤„ç†{processed_count}æ¡æ—¶é—´è®°å½•")
    
    return df_processed


def convert_cer_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    è½¬æ¢CERæ•°æ®ä¸­çš„æ•°å€¼åˆ—
    
    Args:
        df: DataFrame
        
    Returns:
        æ•°å€¼åˆ—è½¬æ¢åçš„DataFrame
    """
    df_processed = df.copy()
    
    # è¯†åˆ«éœ€è¦è½¬æ¢çš„æ•°å€¼åˆ—ï¼ˆå®¹é‡ç›¸å…³ï¼‰
    capacity_columns = []
    for col in df_processed.columns:
        col_lower = col.lower()
        if any(keyword in col_lower for keyword in ['capacity', 'mw', 'installed']):
            capacity_columns.append(col)
    
    if not capacity_columns:
        print("  âš ï¸æœªæ‰¾åˆ°éœ€è¦è½¬æ¢çš„å®¹é‡åˆ—")
        return df_processed
    
    
    converted_count = 0
    
    for col in capacity_columns:
        # è½¬æ¢æ•°å€¼ï¼ˆä½¿ç”¨é€šç”¨å‡½æ•°ï¼‰
        original_values = df_processed[col].copy()
        df_processed[col] = df_processed[col].apply(lambda x: clean_numeric_value(x, 'capacity'))
        
        # ç»Ÿè®¡æˆåŠŸè½¬æ¢çš„æ•°é‡
        success_count = df_processed[col].notna().sum()
        original_count = original_values.notna().sum()
        
        if success_count > 0:
            converted_count += success_count
            print(f"  âœ“æ•°å€¼è½¬æ¢: {col} ({success_count}/{original_count}æ¡è®°å½•)")
    
    if converted_count > 0:
        print(f"  âœ“CERæ•°å€¼è½¬æ¢å®Œæˆ: å…±è½¬æ¢{converted_count}ä¸ªæ•°å€¼")
    else:
        print("  âš ï¸æœªæˆåŠŸè½¬æ¢ä»»ä½•æ•°å€¼")
    
    return df_processed


def process_cer_data_with_cleaning(df: pd.DataFrame, table_type: str) -> pd.DataFrame:
    """
    å¤„ç†CERæ•°æ®ï¼ŒåŒ…æ‹¬åˆ—åè§„èŒƒåŒ–ã€æ—¶é—´å¤„ç†å’Œæ•°å€¼è½¬æ¢
    
    Args:
        df: åŸå§‹DataFrame
        table_type: è¡¨ç±»å‹ï¼ˆå¦‚ "approved_power_stations"ï¼‰
        
    Returns:
        å¤„ç†åçš„DataFrame
    """
    print(f"  ğŸ§¹å¼€å§‹CERæ•°æ®æ¸…ç†: {table_type}")
    
    # 1. åˆ—åè§„èŒƒåŒ–
    print("  ğŸ“è§„èŒƒåŒ–åˆ—å...")
    df_processed = normalize_cer_column_names(df)
    
    # 2. æ—¶é—´å¤„ç†
    print("  ğŸ•å¤„ç†æ—¶é—´åˆ—...")
    df_processed = process_cer_time_columns(df_processed)
    
    # 3. æ•°å€¼è½¬æ¢
    print("  ğŸ”¢è½¬æ¢æ•°å€¼åˆ—...")
    df_processed = convert_cer_numeric_columns(df_processed)
    
    print(f"  âœ“CERæ•°æ®æ¸…ç†å®Œæˆ: {table_type} ({df_processed.shape})")
    
    return df_processed


# =============================================================================
# æ•°æ®è´¨é‡ä¿®å¤åŠŸèƒ½ (åŸ data_quality_fixer.py)
# =============================================================================

def fix_missing_values(df: pd.DataFrame, missing_indicators: List[str] = None) -> pd.DataFrame:
    """
    ç»Ÿä¸€å¤„ç†ç¼ºå¤±å€¼æ ‡è¯†
    
    Args:
        df: åŸå§‹DataFrame
        missing_indicators: ç¼ºå¤±å€¼æ ‡è¯†åˆ—è¡¨
        
    Returns:
        ä¿®å¤åçš„DataFrame
    """
    if missing_indicators is None:
        missing_indicators = MISSING_VALUE_INDICATORS
    
    df_fixed = df.copy()
    
    # ç»Ÿè®¡ä¿®å¤æƒ…å†µ
    fix_count = 0
    
    for col in df_fixed.columns:
        if df_fixed[col].dtype == 'object':  # åªå¤„ç†æ–‡æœ¬åˆ—
            for indicator in missing_indicators:
                # ç²¾ç¡®åŒ¹é…ç¼ºå¤±å€¼æ ‡è¯†
                mask = df_fixed[col].astype(str).str.strip() == indicator
                count = mask.sum()
                if count > 0:
                    df_fixed.loc[mask, col] = None
                    fix_count += count
    
    print(f"  âœ“ç¼ºå¤±å€¼ä¿®å¤: å…±ä¿®å¤ {fix_count} ä¸ªç¼ºå¤±å€¼æ ‡è¯†")
    return df_fixed


def parse_date_flexible(date_str: str) -> Optional[Tuple[int, int, int]]:
    """
    çµæ´»è§£æå¤šç§æ—¥æœŸæ ¼å¼
    
    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸²
        
    Returns:
        (year, month, day) å…ƒç»„ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å›None
    """
    if not date_str or pd.isna(date_str):
        return None
    
    date_str = str(date_str).strip()
    
    # ä½¿ç”¨é€šç”¨æœˆä»½æ˜ å°„
    month_abbr = MONTH_ABBR_TO_NUM
    
    try:
        # æ ¼å¼1: DD/MM/YYYY
        if '/' in date_str:
            parts = date_str.split('/')
            if len(parts) == 3:
                day, month, year = int(parts[0]), int(parts[1]), int(parts[2])
                return (year, month, day)
        
        # æ ¼å¼2: MMM-YYYY
        elif '-' in date_str and len(date_str.split('-')) == 2:
            parts = date_str.split('-')
            month_str, year_str = parts[0].strip().lower(), parts[1].strip()
            if month_str in month_abbr:
                year, month = int(year_str), month_abbr[month_str]
                return (year, month, 1)  # é»˜è®¤ä¸ºæœˆåˆ
        
        # æ ¼å¼3: YYYY-MM-DD
        elif '-' in date_str and len(date_str.split('-')) == 3:
            parts = date_str.split('-')
            if len(parts[0]) == 4:  # å¹´ä»½åœ¨å‰
                year, month, day = int(parts[0]), int(parts[1]), int(parts[2])
                return (year, month, day)
        
        # æ ¼å¼4: DD-MM-YYYY
        elif '-' in date_str and len(date_str.split('-')) == 3:
            parts = date_str.split('-')
            if len(parts[2]) == 4:  # å¹´ä»½åœ¨å
                day, month, year = int(parts[0]), int(parts[1]), int(parts[2])
                return (year, month, day)
    
    except (ValueError, IndexError):
        pass
    
    return None


def fix_date_formats(df: pd.DataFrame) -> pd.DataFrame:
    """
    ä¿®å¤å¤šç§æ—¥æœŸæ ¼å¼
    
    Args:
        df: æ•°æ®DataFrame
        
    Returns:
        ä¿®å¤åçš„DataFrame
    """
    df_fixed = df.copy()
    
    print("  ğŸ“…ä¿®å¤æ—¥æœŸæ ¼å¼...")
    
    # æŸ¥æ‰¾æ‰€æœ‰æ—¥æœŸåˆ—
    date_columns = [col for col in df_fixed.columns if 'date' in col.lower()]
    
    total_fixed = 0
    
    for date_col in date_columns:
        if date_col not in df_fixed.columns:
            continue
            
        print(f"    å¤„ç†æ—¥æœŸåˆ—: {date_col}")
        
        # åˆ›å»ºæ ‡å‡†åŒ–çš„æ—¥æœŸåˆ—
        year_col = f"{date_col}_year_fixed"
        month_col = f"{date_col}_month_fixed"
        day_col = f"{date_col}_day_fixed"
        iso_col = f"{date_col}_iso"
        
        years, months, days, iso_dates = [], [], [], []
        success_count = 0
        
        for idx, date_val in enumerate(df_fixed[date_col]):
            parsed = parse_date_flexible(date_val)
            
            if parsed:
                year, month, day = parsed
                years.append(year)
                months.append(month)
                days.append(day)
                
                # ç”ŸæˆISOæ ¼å¼æ—¥æœŸ
                try:
                    iso_date = f"{year:04d}-{month:02d}-{day:02d}"
                    iso_dates.append(iso_date)
                    success_count += 1
                except:
                    iso_dates.append(None)
            else:
                years.append(None)
                months.append(None)
                days.append(None)
                iso_dates.append(None)
        
        # æ·»åŠ æ–°åˆ—
        df_fixed[year_col] = years
        df_fixed[month_col] = months
        df_fixed[day_col] = days
        df_fixed[iso_col] = iso_dates
        
        total_fixed += success_count
        print(f"      âœ“æˆåŠŸè§£æ {success_count}/{len(df_fixed)} ä¸ªæ—¥æœŸ")
    
    if total_fixed > 0:
        print(f"  âœ“æ—¥æœŸæ ¼å¼ä¿®å¤å®Œæˆ: å…±ä¿®å¤ {total_fixed} ä¸ªæ—¥æœŸå€¼")
    else:
        print(f"  âš ï¸æœªæ‰¾åˆ°éœ€è¦ä¿®å¤çš„æ—¥æœŸåˆ—")
    
    return df_fixed


def fix_nger_specific_issues(df: pd.DataFrame) -> pd.DataFrame:
    """
    ä¿®å¤NGERæ•°æ®ç‰¹æœ‰çš„é—®é¢˜
    
    Args:
        df: NGERæ•°æ®DataFrame
        
    Returns:
        ä¿®å¤åçš„DataFrame
    """
    df_fixed = df.copy()
    
    print("  ğŸ”§ä¿®å¤NGERç‰¹æœ‰é—®é¢˜...")
    
    # 1. ç»Ÿä¸€ç¼ºå¤±å€¼å¤„ç†
    df_fixed = fix_missing_values(df_fixed)
    
    # 2. æ ‡å‡†åŒ–å¸ƒå°”å­—æ®µ
    if 'gridconnected' in df_fixed.columns:
        def standardize_grid_connected(value):
            if pd.isna(value) or value is None:
                return None
            
            val_str = str(value).strip().lower()
            if val_str in ['on', 'connected', 'yes', 'true', '1']:
                return 'Connected'
            elif val_str in ['off', 'disconnected', 'no', 'false', '0']:
                return 'Disconnected'
            else:
                return None
        
        original_count = df_fixed['gridconnected'].notna().sum()
        df_fixed['gridconnected'] = df_fixed['gridconnected'].apply(standardize_grid_connected)
        fixed_count = df_fixed['gridconnected'].notna().sum()
        print(f"    - gridconnectedå­—æ®µæ ‡å‡†åŒ–: {original_count} â†’ {fixed_count}")
    
    # 3. æ ‡å‡†åŒ–ç‡ƒæ–™ç±»å‹
    if 'primaryfuel' in df_fixed.columns:
        df_fixed['primaryfuel'] = df_fixed['primaryfuel'].apply(standardize_fuel_type)
        print(f"    - primaryfuelå­—æ®µæ ‡å‡†åŒ–å®Œæˆ")
    
    # 4. æ¸…ç†è®¾æ–½åç§°
    if 'facilityname' in df_fixed.columns:
        df_fixed['facilityname'] = df_fixed['facilityname'].apply(lambda x: clean_facility_name(x, 'facility'))
        print(f"    - facilitynameå­—æ®µæ¸…ç†å®Œæˆ")
    
    return df_fixed


def fix_cer_specific_issues(df: pd.DataFrame, table_type: str) -> pd.DataFrame:
    """
    ä¿®å¤CERæ•°æ®ç‰¹æœ‰çš„é—®é¢˜
    
    Args:
        df: CERæ•°æ®DataFrame
        table_type: è¡¨ç±»å‹
        
    Returns:
        ä¿®å¤åçš„DataFrame
    """
    df_fixed = df.copy()
    
    print(f"  ğŸ”§ä¿®å¤CERç‰¹æœ‰é—®é¢˜: {table_type}...")
    
    # 1. ç»Ÿä¸€ç¼ºå¤±å€¼å¤„ç†
    df_fixed = fix_missing_values(df_fixed)
    
    # 2. ä¿®å¤æ—¥æœŸæ ¼å¼
    df_fixed = fix_date_formats(df_fixed)
    
    # 3. æ¸…ç†ç”µç«™/é¡¹ç›®åç§°
    name_columns = []
    if 'power_station_name' in df_fixed.columns:
        name_columns.append('power_station_name')
    elif 'Power station name' in df_fixed.columns:
        name_columns.append('Power station name')
    if 'project_name' in df_fixed.columns:
        name_columns.append('project_name')
    elif 'Project Name' in df_fixed.columns:
        name_columns.append('Project Name')
    
    for name_col in name_columns:
        df_fixed[name_col] = df_fixed[name_col].apply(lambda x: clean_facility_name(x, 'station'))
        print(f"    - {name_col}å­—æ®µæ¸…ç†å®Œæˆ")
    
    # 4. æ ‡å‡†åŒ–ç‡ƒæ–™ç±»å‹
    fuel_columns = []
    if 'fuel_source' in df_fixed.columns:
        fuel_columns.append('fuel_source')
    elif 'Fuel Source' in df_fixed.columns:
        fuel_columns.append('Fuel Source')
    elif 'Fuel Source (s)' in df_fixed.columns:
        fuel_columns.append('Fuel Source (s)')
    
    for fuel_col in fuel_columns:
        df_fixed[fuel_col] = df_fixed[fuel_col].apply(standardize_fuel_type)
        print(f"    - {fuel_col}å­—æ®µæ ‡å‡†åŒ–å®Œæˆ")
    
    return df_fixed


def process_data_quality_fixes(df: pd.DataFrame, data_type: str, **kwargs) -> pd.DataFrame:
    """
    ç»Ÿä¸€çš„æ•°æ®è´¨é‡ä¿®å¤å…¥å£å‡½æ•°
    
    Args:
        df: åŸå§‹DataFrame
        data_type: æ•°æ®ç±»å‹ ('nger', 'cer', 'abs')
        **kwargs: é¢å¤–å‚æ•°
        
    Returns:
        ä¿®å¤åçš„DataFrame
    """
    print(f"  ğŸ”§å¼€å§‹æ•°æ®è´¨é‡ä¿®å¤: {data_type.upper()}")
    
    if data_type.lower() == 'nger':
        return fix_nger_specific_issues(df)
    elif data_type.lower() == 'cer':
        table_type = kwargs.get('table_type', 'unknown')
        return fix_cer_specific_issues(df, table_type)
    elif data_type.lower() == 'abs':
        # ABSæ•°æ®ä¿®å¤å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
        return fix_missing_values(df)
    else:
        # é»˜è®¤åªå¤„ç†ç¼ºå¤±å€¼
        return fix_missing_values(df)

