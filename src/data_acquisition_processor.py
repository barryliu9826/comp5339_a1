#!/usr/bin/env python3
"""æ•°æ®è·å–å’Œå¤„ç†å·¥å…·"""

# æ ‡å‡†åº“å¯¼å…¥
from pathlib import Path
import time
import re
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
import requests
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import StaleElementReferenceException

# æœ¬åœ°æ¨¡å—å¯¼å…¥
from database_config import *
from geocoding import add_geocoding_to_cer_data, add_geocoding_to_nger_data, save_global_cache, initialize_geocoding_cache
from excel_utils import get_merged_cells, read_merged_headers
from time_format_utils import process_nger_time_format, process_cer_time_format, process_abs_time_format
from data_cleaner import *

# é…ç½®
DATA_DIR = Path(__file__).resolve().parents[1] / "data"
DATA_DIR.mkdir(exist_ok=True)
SESSION = requests.Session()
SESSION.headers.update({"User-Agent": "COMP5339-Assignment1/1.0"})


# ============================================================================
# æ•°æ®è·å–
# ============================================================================

def download_nger_year(year_data, results_queue):
    """ä¸‹è½½NGERæ•°æ®"""
    thread_id = threading.get_ident()
    year_label, url = year_data
    
    try:
        print(f"[çº¿ç¨‹{thread_id}] ä¸‹è½½NGERæ•°æ®: {year_label}...")
        resp = requests.get(url, timeout=120, headers={"User-Agent": "COMP5339-Assignment1/1.0"})
        resp.raise_for_status()
        
        if "json" in resp.headers.get("Content-Type", "") or resp.text.strip().startswith("["):
            data = resp.json()
            df = pd.DataFrame(data) if isinstance(data, list) else pd.DataFrame([data])
            # ç»Ÿä¸€åˆ—åä¸ºå°å†™ï¼Œå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œé¿å…å­—æ®µæ˜ å°„ç¼ºå¤±
            try:
                df.columns = [str(c).strip().lower() for c in df.columns]
            except Exception:
                pass
            
            # åœ¨çº¿ç¨‹ä¸­ç›´æ¥å¤„ç†æ•°æ®åº“æ“ä½œ
            try:
                # 1. æ•°æ®è´¨é‡ä¿®å¤ï¼ˆå…¥åº“å‰ï¼‰
                if df is not None and not df.empty:
                    df_before = df.copy()
                    df = process_data_quality_fixes(df, 'nger')
                    print(f"  âœ“NGERæ•°æ®è´¨é‡ä¿®å¤å®Œæˆ: {year_label}")
                
                # 2. æ—¶é—´æ ¼å¼è½¬æ¢
                if df is not None and not df.empty:
                    df = process_nger_time_format(df, year_label)
                    
                # 3. åœ°ç†ç¼–ç å¢å¼º
                if df is not None and not df.empty:
                    df = add_geocoding_to_nger_data(df, max_workers=1)
            except Exception as e:
                print(f"  âš NGERæ•°æ®å¤„ç†å¤±è´¥ï¼Œç»§ç»­å…¥åº“åŸå§‹æ•°æ®: {e}")

            conn = get_db_connection()
            if conn and df is not None and not df.empty:
                if save_nger_data(conn, year_label, df):
                    results_queue.put((year_label, True, None))
                    print(f"âœ“[çº¿ç¨‹{thread_id}] NGERæ•°æ®ä¸‹è½½å’Œå…¥åº“å®Œæˆ: {year_label}")
                else:
                    results_queue.put((year_label, False, "æ•°æ®åº“å…¥åº“å¤±è´¥"))
            else:
                results_queue.put((year_label, False, "æ•°æ®åº“è¿æ¥å¤±è´¥"))
        else:
            results_queue.put((year_label, False, "æ ¼å¼é”™è¯¯"))
    except Exception as e:
        results_queue.put((year_label, False, str(e)))
        print(f"âœ—[çº¿ç¨‹{thread_id}] NGERæ•°æ®ä¸‹è½½å¤±è´¥: {year_label}: {e}")
    finally:
        if 'conn' in locals() and conn:
            return_db_connection(conn)

def process_threading_results(results_queue, total_tasks, operation_name):
    """å¤„ç†å¤šçº¿ç¨‹ç»“æœ"""
    success_count = 0
    while not results_queue.empty():
        item_label, success, error = results_queue.get()
        if error: 
            print(f"âœ—{item_label}: {error}")
            continue
        if success:
            success_count += 1
    
    print(f"âœ“{operation_name}å¤„ç†å®Œæˆ: {success_count}/{total_tasks} ä¸ªä»»åŠ¡æˆåŠŸ")
    return success_count > 0

def fetch_nger_data(max_workers=1):
    """å¤šçº¿ç¨‹è·å–NGERæ•°æ®"""
    try:
        table = pd.read_csv(DATA_DIR / "nger_data_api_links.csv")
        year_col = next((c for c in ["year_label", "year"] if c in table.columns), None)
        url_col = next((c for c in ["url", "api_url", "link"] if c in table.columns), None)
        
        if not year_col or not url_col:
            return False
        
        tasks = [(str(row[year_col]).strip(), str(row[url_col]).strip()) 
                 for _, row in table.iterrows() 
                 if str(row[url_col]).lower() != "nan"]
        
        print(f"å¼€å§‹å¤šçº¿ç¨‹ä¸‹è½½NGERæ•°æ® ({max_workers}çº¿ç¨‹): {len(tasks)}ä¸ªå¹´ä»½æ–‡ä»¶")
        
        results_queue = queue.Queue()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(download_nger_year, task, results_queue) for task in tasks]
            [f.result() for f in as_completed(futures)]
        
        return process_threading_results(results_queue, len(tasks), "NGERæ•°æ®")
        
    except Exception as e:
        print(f"âœ—NGERæ•°æ®å¤„ç†å¤±è´¥: {e}")
        return False

def fetch_abs_data():
    """ä¸‹è½½ABSæ•°æ®"""
    url = "https://www.abs.gov.au/methodologies/data-region-methodology/2011-24/14100DO0003_2011-24.xlsx"
    filepath = DATA_DIR / "14100DO0003_2011-24.xlsx"
    
    try:
        response = SESSION.get(url, stream=True, headers={'User-Agent': 'Mozilla/5.0'})
        response.raise_for_status()
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(8192):
                if chunk: f.write(chunk)
        print("âœ“ABSæ•°æ®ä¸‹è½½æˆåŠŸ")
        return filepath
    except Exception as e:
        print(f"âœ—ABSæ•°æ®ä¸‹è½½å¤±è´¥: {e}")
        return None

# ============================================================================
# CERçˆ¬å–
# ============================================================================

def setup_driver():
    """è®¾ç½®WebDriver"""
    options = Options()
    for arg in ["--headless", "--no-sandbox", "--disable-dev-shm-usage", "--disable-gpu"]:
        options.add_argument(arg)
    options.add_argument("--window-size=1920,1080")
    
    try:
        return webdriver.Chrome(options=options)
    except Exception as e:
        print(f"âœ—WebDriver: {e}")
        return None

def identify_table(df):
    """è¯†åˆ«è¡¨ç±»å‹"""
    cols = ' '.join(df.columns).lower()
    if 'accreditation code' in cols and 'power station name' in cols:
        return "approved_power_stations"
    elif 'project name' in cols and 'committed date' in cols:
        return "committed_power_stations"
    elif 'project name' in cols and 'mw capacity' in cols:
        return "probable_power_stations"
    return None

def parse_table(table_element):
    """è§£æè¡¨æ ¼"""
    try:
        rows = []
        table_rows = table_element.find_elements(By.TAG_NAME, "tr")
        if not table_rows:
            return pd.DataFrame()
        
        headers = [th.text.strip() for th in table_rows[0].find_elements(By.TAG_NAME, "th")]
        if not headers:
            return pd.DataFrame()
        
        for row in table_rows[1:]:
            cells = row.find_elements(By.TAG_NAME, "td")
            if cells:
                row_data = [cell.text.strip() for cell in cells]
                if len(row_data) == len(headers):
                    rows.append(row_data)
        
        if not rows:
            return pd.DataFrame()
        
        df = pd.DataFrame(rows, columns=headers)
        
        # è¿‡æ»¤ç©ºåˆ—
        keep_cols = []
        for i, col in enumerate(df.columns):
            col_name = str(col).strip()
            if col_name and col_name.lower() not in ['', 'nan', 'none']:
                col_data = df.iloc[:, i].astype(str).str.strip()
                if not col_data.isin(['', 'nan', 'None']).all():
                    keep_cols.append(col)
        
        return df[keep_cols] if keep_cols else pd.DataFrame()
    except Exception as e:
        print(f"âœ—è§£æ: {e}")
        return pd.DataFrame()

def get_max_pages(table_element):
    """è·å–æœ€å¤§é¡µæ•°"""
    try:
        container = table_element.find_element(By.XPATH, "ancestor::*[.//table][1]")
        elements = container.find_elements(By.XPATH, ".//*[contains(text(), 'Showing') and contains(text(), 'of')]")
        if elements:
            match = re.search(r'showing\s*(\d+)\s*to\s*(\d+)\s*of\s*(\d+)', elements[0].text, re.IGNORECASE)
            if match:
                start, end, total = map(int, match.groups())
                return (total + (end - start)) // (end - start + 1)
        return 10
    except:
        return 10

def scrape_paginated_table(driver, table_element, table_type):
    """çˆ¬å–åˆ†é¡µè¡¨"""
    max_pages, frames, page = get_max_pages(table_element), [], 1
    print(f"{table_type}(æœ€å¤§{max_pages}é¡µ)")
    
    while page <= max_pages:
        try:
            df = parse_table(table_element)
            if not df.empty: 
                frames.append(df)
                print(f"  ç¬¬{page}é¡µ: {len(df)}è¡Œ")
            
            if page < max_pages:
                try:
                    container = table_element.find_element(By.XPATH, "ancestor::*[.//table][1]")
                    next_btn = container.find_element(By.XPATH, ".//button[contains(text(), 'â€º') or contains(text(), 'Next')]")
                    if next_btn.is_displayed() and next_btn.is_enabled():
                        driver.execute_script("arguments[0].click();", next_btn)
                        time.sleep(3)
                        page += 1
                    else: break
                except: break
            else: break
                
        except StaleElementReferenceException:
            time.sleep(2)
            try:
                tables = driver.find_elements(By.TAG_NAME, "table")
                table_element = tables[0]
            except:
                break
        except Exception as e:
            print(f"  é¡µ{page}é”™è¯¯: {e}")
            break
    
    if frames:
        result = pd.concat(frames, ignore_index=True).drop_duplicates()
        print(f"  âœ“{len(result)}è¡Œ")
        return result
    return pd.DataFrame()

def fetch_cer_data(max_workers=1):
    """çˆ¬å–CERæ•°æ®"""
    driver = setup_driver()
    if not driver:
        return False
    
    conn = get_db_connection()
    if not conn:
        driver.quit()
        return False
    
    try:
        driver.get("https://cer.gov.au/markets/reports-and-data/large-scale-renewable-energy-data")
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, "table")))
        time.sleep(5)
        
        tables = driver.find_elements(By.TAG_NAME, "table")
        targets = ["approved_power_stations", "committed_power_stations", "probable_power_stations"]
        success_count = 0
        
        print(f"å‘ç°CERç½‘é¡µè¡¨æ ¼: {len(tables)}ä¸ª")
        
        for i, table in enumerate(tables):
            try:
                df_temp = parse_table(table)
                if df_temp.empty: continue
                
                table_type = identify_table(df_temp)
                if table_type not in targets: continue
                
                print(f"\nå¼€å§‹å¤„ç†CERè¡¨æ ¼: {table_type}...")
                df_result = scrape_paginated_table(driver, table, table_type)
                if df_result.empty: continue
                
                # æ•°æ®æ¸…ç†ï¼šåˆ—åè§„èŒƒåŒ–ã€æ—¶é—´å¤„ç†ã€æ•°å€¼è½¬æ¢ï¼ˆåœ¨åœ°ç†ç¼–ç å‰å®Œæˆï¼‰
                print(f"  ğŸ§¹å¼€å§‹CERæ•°æ®æ¸…ç†: {table_type}...")
                
                # 1. åŸºç¡€æ•°æ®æ¸…ç†ï¼ˆåˆ—åè§„èŒƒåŒ–ã€æ—¶é—´å¤„ç†ã€æ•°å€¼è½¬æ¢ï¼‰
                df_cleaned = process_cer_data_with_cleaning(df_result, table_type)
                
                # 2. æ•°æ®è´¨é‡ä¿®å¤ï¼ˆç¼ºå¤±å€¼ã€æ—¥æœŸæ ¼å¼ç»Ÿä¸€ï¼‰
                df_fixed = process_data_quality_fixes(df_cleaned, 'cer', table_type=table_type)
                
                print(f"  âœ“CERæ•°æ®æ¸…ç†å’Œä¿®å¤å®Œæˆ: {table_type}")
                df_time_processed = df_fixed
                
                print(f"  å¯¹CERæ•°æ®è¿›è¡Œå¤šçº¿ç¨‹åœ°ç†ç¼–ç ...")
                df_geocoded = add_geocoding_to_cer_data(df_time_processed, table_type, max_workers)
                
                if save_cer_data(conn, table_type, df_geocoded):
                    success_count += 1
                    print(f"  âœ“CERè¡¨æ ¼å¤„ç†å®Œæˆ: {table_type}")
                    
            except Exception as e:
                print(f"  âœ—CERè¡¨æ ¼{i+1}å¤„ç†å¤±è´¥: {e}")
        
        print(f"âœ“CERæ•°æ®å¤„ç†å®Œæˆ: {success_count}ä¸ªè¡¨æ ¼æˆåŠŸå…¥åº“")
        return success_count > 0
        
    except Exception as e:
        print(f"âœ—CERæ•°æ®å¤„ç†å¤±è´¥: {e}")
        return False
    finally:
        if conn:
            return_db_connection(conn)
        driver.quit()

# ============================================================================
# Excelå¤„ç†
# ============================================================================


def process_abs_merged_cell_with_db(args):
    """å¤„ç†ABSå•å…ƒæ ¼å¹¶å…¥åº“ï¼ˆæ•°æ®å·²é¢„æ¸…ç†ï¼‰"""
    thread_id = threading.get_ident()
    cell, df_cleaned, level_info, db_config, column_types = args
    
    try:
        print(f"[çº¿ç¨‹{thread_id}] å¤„ç†ABSåˆå¹¶å•å…ƒæ ¼: {cell['value']}")
        conn = get_db_connection()
        
        if not conn:
            return {'success': False, 'cell_name': cell['value'], 'thread_id': thread_id, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}
        
        start_col, end_col = cell['start_col'] - 1, cell['end_col']
        selected_cols = ['Code', 'Label', 'Year'] + list(df_cleaned.columns[start_col:end_col])
        subset_df = df_cleaned[selected_cols].copy()
        
        # æå–è¯¥å­é›†ç›¸å…³çš„åˆ—ç±»å‹ä¿¡æ¯
        subset_column_types = {col: column_types.get(col, 'text') for col in selected_cols[3:]}
        
        table_name = create_abs_table_with_types(conn, cell['value'], selected_cols, subset_column_types)
        if table_name and insert_abs_data_cleaned(conn, table_name, subset_df, level_info['level'], subset_column_types):
            print(f"âœ“[çº¿ç¨‹{thread_id}] ABSæ•°æ®å…¥åº“æˆåŠŸ: {cell['value']}")
            return {'success': True, 'cell_name': cell['value'], 'thread_id': thread_id, 'error': None}
        else:
            print(f"âœ—[çº¿ç¨‹{thread_id}] ABSæ•°æ®å…¥åº“å¤±è´¥: {cell['value']}")
            return {'success': False, 'cell_name': cell['value'], 'thread_id': thread_id, 'error': 'å…¥åº“å¤±è´¥'}
        
    except Exception as e:
        print(f"âœ—[çº¿ç¨‹{thread_id}] ABSå•å…ƒæ ¼å¤„ç†å¤±è´¥: {cell['value']}: {e}")
        return {'success': False, 'cell_name': cell['value'], 'thread_id': thread_id, 'error': str(e)}
    finally:
        if 'conn' in locals() and conn:
            return_db_connection(conn)

def run_threading_tasks(tasks, task_func, max_workers, operation_name):
    """è¿è¡Œå¤šçº¿ç¨‹ä»»åŠ¡çš„é€šç”¨å‡½æ•°"""
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(task_func, task): task[0]['value'] if isinstance(task[0], dict) else str(task[0]) for task in tasks}
        for future in as_completed(futures):
            try:
                results.append(future.result())
            except Exception as e:
                task_name = futures[future]
                print(f"âœ—çº¿ç¨‹å¼‚å¸¸{task_name}: {e}")
                results.append({'success': False, 'cell_name': task_name, 'error': str(e)})
    
    success_count = sum(1 for r in results if r['success'])
    print(f"âœ“{operation_name}å¤„ç†å®Œæˆ: {success_count}/{len(tasks)} ä¸ªä»»åŠ¡æˆåŠŸ")
    return results

def process_abs_data(file_path: str, max_workers=4):
    """å¤šçº¿ç¨‹å¤„ç†ABSæ•°æ®"""
    try:
        # åœ°ç†çº§åˆ«å®šä¹‰
        levels = {
            "Table 1": {"desc": "å·çº§", "level": 0},
            "Table 2": {"desc": "åœ°æ–¹æ”¿åºœçº§", "level": 1}
        }
        
        for sheet_name in ["Table 1", "Table 2"]:
            level_info = levels[sheet_name]
            print(f"\nå¼€å§‹å¤„ç†ABSè¡¨æ ¼: {sheet_name}({level_info['desc']})...")
            
            merged_cells = get_merged_cells(file_path, sheet_name)
            df = read_merged_headers(file_path, sheet_name)
            print(f"å‘ç°{len(merged_cells)}ä¸ªåˆå¹¶å•å…ƒæ ¼, æ•°æ®{df.shape[0]}è¡Œ")
            
            # å¤„ç†ABSæ—¶é—´æ ¼å¼ï¼ˆéªŒè¯ï¼‰
            df = process_abs_time_format(df)
            
            # æ•°æ®æ¸…ç†ï¼šæ•°å€¼è½¬æ¢å’ŒLGAæ ‡å‡†åŒ–ï¼ˆåœ¨å…¥åº“å‰å®Œæˆï¼‰
            print(f"  ğŸ§¹å¼€å§‹{sheet_name}æ•°æ®æ¸…ç†...")
            df_cleaned, column_types = process_abs_data_with_cleaning(df)
            
            # ç»Ÿè®¡æ¸…ç†ç»“æœ
            numeric_cols = {k: v for k, v in column_types.items() if v != 'text'}
            if numeric_cols:
                print(f"  ğŸ“Š{sheet_name}: æ£€æµ‹å¹¶è½¬æ¢äº†{len(numeric_cols)}ä¸ªæ•°å€¼åˆ—")
            print(f"  âœ“{sheet_name}æ•°æ®æ¸…ç†å®Œæˆ")
            
            tasks = [(cell, df_cleaned, level_info, DB_CONFIG, column_types) for cell in merged_cells]
            print(f"ä½¿ç”¨{max_workers}ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†ABSæ•°æ®...")
            
            results = run_threading_tasks(tasks, process_abs_merged_cell_with_db, max_workers, f"ABSè¡¨æ ¼{sheet_name}")
            
            # çº¿ç¨‹ç»Ÿè®¡
            thread_stats = {}
            for r in results:
                tid = r.get('thread_id', 'Unknown')
                if tid not in thread_stats:
                    thread_stats[tid] = {'success': 0, 'failed': 0}
                if r['success']:
                    thread_stats[tid]['success'] += 1
                else:
                    thread_stats[tid]['failed'] += 1
            
            print("ABSæ•°æ®å¤„ç†çº¿ç¨‹æ‰§è¡Œç»Ÿè®¡:")
            for tid, stats in thread_stats.items():
                print(f"  çº¿ç¨‹{tid}: {stats['success']}/{stats['success']+stats['failed']}")
        
        print("âœ“ABSæ•°æ®å¤„ç†å…¨éƒ¨å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âœ—ABSæ•°æ®å¤„ç†å¤±è´¥: {e}")
        return False

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def create_table_direct(table_name: str, create_func, *args):
    """ç›´æ¥åˆ›å»ºè¡¨"""
    conn = None
    try:
        conn = get_db_connection()
        if not conn:
            print("âœ—æ•°æ®åº“è¿æ¥å¤±è´¥")
            return False
        
        if args:
            return create_func(conn, *args)
        else:
            return create_func(conn)
    except Exception as e:
        print(f"âœ—åˆ›å»º{table_name}å¤±è´¥: {e}")
        return False
    finally:
        if conn:
            return_db_connection(conn)

def print_final_results(results):
    """æ‰“å°æœ€ç»ˆç»“æœ"""
    success = sum(results)
    print(f"\n{'='*50}")
    print(f"âœ“æ•°æ®å¤„ç†ç³»ç»Ÿæ‰§è¡Œå®Œæˆ: {success}/3 ä¸ªæ¨¡å—æˆåŠŸ")
    nger_symbol = 'âœ“' if results[0] else 'âœ—'
    abs_symbol = 'âœ“' if results[1] else 'âœ—'
    cer_symbol = 'âœ“' if results[2] else 'âœ—'
    print(f"NGER: {nger_symbol}  |  ABS: {abs_symbol}  |  CER: {cer_symbol}")
    print(f"{'='*50}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print("æ•°æ®è·å–å’Œå¤„ç†ç³»ç»Ÿ")
    print("NGERæ•°æ® + ABSç»æµæ•°æ® + CERç”µç«™æ•°æ®")
    print("=" * 50)
    
    # åˆå§‹åŒ–åœ°ç†ç¼–ç ç¼“å­˜ï¼ˆæå‰åŠ è½½ç¼“å­˜ï¼Œå‡å°‘APIè°ƒç”¨ä¸ç£ç›˜I/Oç«äº‰ï¼‰
    try:
        initialize_geocoding_cache(str(DATA_DIR / "geocoding_cache.json"))
    except Exception as e:
        print(f"âš åœ°ç†ç¼–ç ç¼“å­˜åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # åˆå§‹åŒ–è¿æ¥æ± 
    pool = get_connection_pool(minconn=2, maxconn=15)
    if not pool:
        print("âœ—æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥")
        return
    
    try:
        print("å…ˆåˆ›å»ºæ•°æ®è¡¨ï¼Œå†è·å–ã€å¤„ç†æ•°æ®...")
        
        # # åˆ›å»ºNGERè¡¨
        # print("\n" + "=" * 20 + " 1. åˆ›å»ºNGERè¡¨ " + "=" * 20)
        # nger_table_ok = create_table_direct("NGERè¡¨", create_nger_table)
        # if not nger_table_ok:
        #     print("âœ—NGERè¡¨å‡†å¤‡å¤±è´¥")
        #     return
        
        # # NGERæ•°æ®è·å–å’Œå¤„ç†
        # print("\n" + "=" * 20 + " 2. NGERæ•°æ®è·å–å’Œå¤„ç† " + "=" * 20)
        # nger_ok = fetch_nger_data(max_workers=1)
        
        # # åˆ›å»ºCERè¡¨
        # print("\n" + "=" * 20 + " 3. åˆ›å»ºCERè¡¨ " + "=" * 20)
        # cer_table_ok = create_table_direct("CERè¡¨", create_cer_tables)
        # if not cer_table_ok:
        #     print("âœ—CERè¡¨å‡†å¤‡å¤±è´¥")
        #     return
        
        # # CERç”µç«™æ•°æ®è·å–å’Œå¤„ç†
        # print("\n" + "=" * 20 + " 4. CERç”µç«™æ•°æ®è·å–å’Œå¤„ç† " + "=" * 20)
        # cer_ok = fetch_cer_data()
    
        # ABSæ•°æ®å¤„ç†
        abs_file = fetch_abs_data()
        if abs_file:
            print("\n" + "=" * 20 + " 5. åˆ›å»ºABSè¡¨ " + "=" * 20)
            abs_table_ok = create_table_direct("ABSè¡¨", create_all_abs_tables, str(abs_file))
            if not abs_table_ok:
                print("âœ—ABSè¡¨å‡†å¤‡å¤±è´¥")
                return
            
            print("\n" + "=" * 20 + " 6. ABSç»æµæ•°æ®è·å–å’Œå¤„ç† " + "=" * 20)
            abs_ok = process_abs_data(str(abs_file), max_workers=10)
        else:
            abs_ok = False
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        # print_final_results([nger_ok, abs_ok, cer_ok])
        
        # ä¿å­˜åœ°ç†ç¼–ç ç¼“å­˜
        print("æ­£åœ¨ä¿å­˜åœ°ç†ç¼–ç ç¼“å­˜...")
        try:
            save_global_cache()
            print("âœ“åœ°ç†ç¼–ç ç¼“å­˜å·²ä¿å­˜")
        except Exception as e:
            print(f"âœ—ä¿å­˜åœ°ç†ç¼–ç ç¼“å­˜å¤±è´¥: {e}")
        
    finally:
        # å…³é—­è¿æ¥æ± 
        close_connection_pool()

if __name__ == "__main__":
    main()